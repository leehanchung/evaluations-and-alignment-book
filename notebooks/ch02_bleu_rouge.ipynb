{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: BLEU and ROUGE\n",
    "\n",
    "**Hands-on implementation of n-gram metrics for evaluating generated text.**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand n-grams** - The fundamental building blocks of text comparison used by BLEU and ROUGE\n",
    "2. **Implement BLEU from scratch** - Including modified precision, geometric mean, and brevity penalty\n",
    "3. **Implement ROUGE from scratch** - Including ROUGE-N and ROUGE-L variants\n",
    "4. **Recognize gaming vulnerabilities** - How naive metrics can be exploited and how BLEU/ROUGE defend against this\n",
    "5. **Apply these metrics in practice** - Using the `evaluate` library for production use cases\n",
    "\n",
    "## Why These Metrics Matter\n",
    "\n",
    "When we build systems that generate text (machine translation, summarization, chatbots), we need ways to automatically measure quality. Human evaluation is the gold standard, but it's expensive and slow.\n",
    "\n",
    "**BLEU** (Bilingual Evaluation Understudy) and **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) are the foundational automatic metrics that have been used for decades:\n",
    "\n",
    "- **BLEU** (2002): Originally designed for machine translation, asks \"How much of the machine output is correct?\"\n",
    "- **ROUGE** (2004): Originally designed for summarization, asks \"How much of the reference content is captured?\"\n",
    "\n",
    "These metrics aren't perfect, but understanding them deeply teaches you:\n",
    "- How to think about text similarity mathematically\n",
    "- The tradeoffs between precision and recall\n",
    "- How to design metrics that resist gaming\n",
    "\n",
    "Let's build them from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Building BLEU from Scratch\n",
    "\n",
    "We'll start by importing the basic tools we need. BLEU is fundamentally about counting and comparing word sequences, so we only need `Counter` for counting and `math` for logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-grams: The Building Blocks\n",
    "\n",
    "An **n-gram** is a contiguous sequence of n tokens (usually words) from a text. N-grams are the fundamental unit of comparison in both BLEU and ROUGE.\n",
    "\n",
    "| N | Name | Example from \"the cat sat\" |\n",
    "|---|------|---------------------------|\n",
    "| 1 | Unigram | \"the\", \"cat\", \"sat\" |\n",
    "| 2 | Bigram | \"the cat\", \"cat sat\" |\n",
    "| 3 | Trigram | \"the cat sat\" |\n",
    "\n",
    "**Why n-grams?** They capture local word order and phrase structure:\n",
    "- **Unigrams** tell us if the right words are present (vocabulary coverage)\n",
    "- **Bigrams** tell us if word pairs are correct (basic grammar/fluency)\n",
    "- **Higher n-grams** capture longer phrases (more sophisticated structure)\n",
    "\n",
    "The key insight: if a translation has the right unigrams but wrong bigrams, it likely has the right words in the wrong order!\n",
    "\n",
    "Let's implement n-gram extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens: list[str], n: int) -> list[tuple]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Simple whitespace tokenizer with lowercasing.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Example from the book\n",
    "sentence = \"The cat is on the mat\"\n",
    "tokens = tokenize(sentence)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Unigrams (n=1): {get_ngrams(tokens, 1)}\")\n",
    "print(f\"Bigrams (n=2): {get_ngrams(tokens, 2)}\")\n",
    "print(f\"Trigrams (n=3): {get_ngrams(tokens, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how n-grams slide over the text like a window. A sentence with $m$ tokens produces $m - n + 1$ n-grams of size $n$. This means:\n",
    "- Longer texts produce more n-grams\n",
    "- Shorter texts may produce zero n-grams for higher values of n (you can't get a 5-gram from a 3-word sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Gaming Problem: Why Naive Precision Fails\n",
    "\n",
    "Before we build BLEU properly, let's understand *why* it was designed the way it was. \n",
    "\n",
    "**Precision** measures: \"What fraction of my output is correct?\"\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{Words in candidate that appear in reference}}{\\text{Total words in candidate}}$$\n",
    "\n",
    "This sounds reasonable, but it can be easily gamed. Consider a pathological translation that just repeats a common word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 from the book\n",
    "candidate = \"the the the the the the the\"\n",
    "reference1 = \"The cat is on the mat.\"\n",
    "reference2 = \"There is a cat on the mat.\"\n",
    "\n",
    "cand_tokens = tokenize(candidate)\n",
    "ref1_tokens = tokenize(reference1)\n",
    "ref2_tokens = tokenize(reference2)\n",
    "\n",
    "# Naive precision: what fraction of candidate words appear in references?\n",
    "ref_words = set(ref1_tokens) | set(ref2_tokens)\n",
    "matches = sum(1 for w in cand_tokens if w in ref_words)\n",
    "naive_precision = matches / len(cand_tokens)\n",
    "\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(f\"Naive precision: {matches}/{len(cand_tokens)} = {naive_precision:.0%}\")\n",
    "print(\"\\n^ This is nonsense but scores 100%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is absurd! A nonsense output of just \"the the the...\" scores 100% precision because \"the\" appears in the references.\n",
    "\n",
    "**The lesson:** Any metric that can be gamed this easily is useless for evaluation. We need a smarter approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modified N-gram Precision: The First Defense\n",
    "\n",
    "BLEU's solution is elegant: **clip each n-gram count by its maximum occurrence in any reference**.\n",
    "\n",
    "The intuition: If \"the\" appears at most 2 times in any reference, then we should only credit at most 2 occurrences of \"the\" in the candidate, no matter how many times it appears.\n",
    "\n",
    "$$\\text{Modified Precision}_n = \\frac{\\sum_{\\text{n-gram}} \\min(\\text{Count}_{\\text{candidate}}, \\text{MaxCount}_{\\text{reference}})}{\\sum_{\\text{n-gram}} \\text{Count}_{\\text{candidate}}}$$\n",
    "\n",
    "This completely defeats the repetition attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate: str, references: list[str], n: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Calculate modified n-gram precision.\n",
    "    Clips counts by max occurrence in any single reference.\n",
    "    \"\"\"\n",
    "    cand_ngrams = get_ngrams(tokenize(candidate), n)\n",
    "    cand_counts = Counter(cand_ngrams)\n",
    "    \n",
    "    # Get max count for each n-gram across all references\n",
    "    max_ref_counts = Counter()\n",
    "    for ref in references:\n",
    "        ref_counts = Counter(get_ngrams(tokenize(ref), n))\n",
    "        for ngram, count in ref_counts.items():\n",
    "            max_ref_counts[ngram] = max(max_ref_counts[ngram], count)\n",
    "    \n",
    "    # Clip candidate counts\n",
    "    clipped_count = sum(\n",
    "        min(count, max_ref_counts.get(ngram, 0))\n",
    "        for ngram, count in cand_counts.items()\n",
    "    )\n",
    "    total_count = sum(cand_counts.values())\n",
    "    \n",
    "    return clipped_count / total_count if total_count > 0 else 0\n",
    "\n",
    "# Test on the gaming example\n",
    "references = [reference1, reference2]\n",
    "mod_precision = modified_precision(candidate, references, n=1)\n",
    "\n",
    "print(f\"Modified precision: {mod_precision:.2f}\")\n",
    "print(f\"'the' appears max 2 times in any reference\")\n",
    "print(f\"So: 2/7 = {2/7:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the gaming attempt gets a much more appropriate score of 0.29 instead of 1.00.\n",
    "\n",
    "**Key insight:** By capping counts at the maximum reference occurrence, we ensure that repeating words provides diminishing returns. You can't score higher than what the reference actually contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real Translation Evaluation\n",
    "\n",
    "Let's apply modified precision to a realistic machine translation example. Here we have a Chinese sentence with three human reference translations, and two candidate machine translations of varying quality.\n",
    "\n",
    "**Chinese source:** 「它是保证军队永远听党指挥的行动指南。」\n",
    "\n",
    "This example demonstrates how n-gram precision at different levels captures different aspects of translation quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 from the book\n",
    "candidate1 = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "candidate2 = \"It is to ensure the troops forever hearing the activity guidebook that party direct.\"\n",
    "\n",
    "ref1 = \"It is a guide to action that ensures that the military will forever heed Party commands.\"\n",
    "ref2 = \"It is the guiding principle which guarantees the military forces always being under the command of the Party.\"\n",
    "ref3 = \"It is the practical guide for the army always to heed the directions of the party.\"\n",
    "\n",
    "references = [ref1, ref2, ref3]\n",
    "\n",
    "# Calculate precision for different n-gram levels\n",
    "print(\"N-gram precisions:\")\n",
    "print(\"-\" * 40)\n",
    "for n in range(1, 5):\n",
    "    p1 = modified_precision(candidate1, references, n)\n",
    "    p2 = modified_precision(candidate2, references, n)\n",
    "    print(f\"{n}-gram: Candidate1={p1:.3f}, Candidate2={p2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Candidate 1 (good) maintains high precision even at higher n-gram levels\n",
    "- Candidate 2 (poor) has decent unigram precision (it uses some right words) but crashes at higher levels (wrong word order, bad phrasing)\n",
    "\n",
    "This is exactly what we want! The metric correctly identifies that Candidate 2 has structural problems even though it contains some correct vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Combining N-gram Levels: Why Geometric Mean?\n",
    "\n",
    "We have precision scores at multiple n-gram levels (1-gram, 2-gram, 3-gram, 4-gram). How do we combine them into a single score?\n",
    "\n",
    "BLEU uses the **geometric mean** rather than the arithmetic mean. The key property is the **zero-product rule**: if any component is zero, the entire product is zero.\n",
    "\n",
    "$$\\text{Geometric Mean} = \\sqrt[n]{p_1 \\times p_2 \\times \\cdots \\times p_n}$$\n",
    "\n",
    "**Why does this matter?** It penalizes systems that completely fail at any n-gram level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def geometric_mean(values):\n",
    "    product = 1\n",
    "    for v in values:\n",
    "        product *= v\n",
    "    return product ** (1 / len(values))\n",
    "\n",
    "# System A: Balanced performance\n",
    "system_a = [0.80, 0.60, 0.40, 0.20]\n",
    "\n",
    "# System B: Degenerate - only unigrams work\n",
    "system_b = [0.80, 0.00, 0.00, 0.00]\n",
    "\n",
    "print(\"System A (balanced): p1=0.80, p2=0.60, p3=0.40, p4=0.20\")\n",
    "print(f\"  Arithmetic mean: {arithmetic_mean(system_a):.3f}\")\n",
    "print(f\"  Geometric mean:  {geometric_mean(system_a):.3f}\")\n",
    "\n",
    "print(\"\\nSystem B (degenerate): p1=0.80, p2=0.00, p3=0.00, p4=0.00\")\n",
    "print(f\"  Arithmetic mean: {arithmetic_mean(system_b):.3f}\")\n",
    "print(f\"  Geometric mean:  {geometric_mean(system_b):.3f}\")\n",
    "print(\"\\n^ Geometric mean = 0 if ANY component fails!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The zero-product property is a feature, not a bug!**\n",
    "\n",
    "System B has zero bigram/trigram/4-gram precision, meaning it produces no correct word sequences at all. With arithmetic mean, it would still score 0.20 - hiding a catastrophic failure.\n",
    "\n",
    "Geometric mean correctly gives it a score of 0, signaling that the system is fundamentally broken despite getting some unigrams right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Brevity Penalty: The Second Defense\n",
    "\n",
    "Modified precision solved the repetition problem, but there's another gaming strategy: **output only high-confidence words**.\n",
    "\n",
    "If a system outputs just \"the military party\" (3 words), every word might be correct (100% precision), but it's clearly not a complete translation. We need to penalize outputs that are too short.\n",
    "\n",
    "**Brevity Penalty (BP):**\n",
    "\n",
    "$$BP = \\begin{cases} 1 & \\text{if } c \\geq r \\\\ e^{(1 - r/c)} & \\text{if } c < r \\end{cases}$$\n",
    "\n",
    "Where $c$ = candidate length, $r$ = reference length.\n",
    "\n",
    "The penalty is exponential: the shorter you are, the more severe the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate_len: int, reference_len: int) -> float:\n",
    "    \"\"\"Calculate BLEU brevity penalty.\"\"\"\n",
    "    if candidate_len >= reference_len:\n",
    "        return 1.0\n",
    "    return math.exp(1 - reference_len / candidate_len)\n",
    "\n",
    "# Gaming example: very short candidate\n",
    "candidate_short = \"the military party\"  # 3 words\n",
    "reference = ref1  # 16 words\n",
    "\n",
    "c_len = len(tokenize(candidate_short))\n",
    "r_len = len(tokenize(reference))\n",
    "\n",
    "bp = brevity_penalty(c_len, r_len)\n",
    "precision = modified_precision(candidate_short, [reference], n=1)\n",
    "\n",
    "print(f\"Candidate: '{candidate_short}' ({c_len} words)\")\n",
    "print(f\"Reference length: {r_len} words\")\n",
    "print(f\"\\nUnigram precision: {precision:.2f} (looks perfect!)\")\n",
    "print(f\"Brevity penalty: {bp:.4f}\")\n",
    "print(f\"Adjusted score: {precision * bp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The brevity penalty in action:** Even with perfect precision, a 3-word output compared to a 16-word reference gets crushed by a penalty of ~0.01. This makes the gaming strategy ineffective.\n",
    "\n",
    "**Why exponential?** Linear penalties would be too gentle. A translation that's half the length should be penalized severely, not just get a 50% reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete BLEU Implementation\n",
    "\n",
    "Now we can put it all together. The complete BLEU formula is:\n",
    "\n",
    "$$\\text{BLEU} = BP \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "Where:\n",
    "- $BP$ = Brevity Penalty\n",
    "- $p_n$ = Modified precision for n-grams of size n\n",
    "- $w_n$ = Weight for each n-gram level (typically uniform: $w_n = 1/N$)\n",
    "- $N$ = Maximum n-gram size (typically 4)\n",
    "\n",
    "The log-sum formulation is equivalent to geometric mean but numerically more stable.\n",
    "\n",
    "**Standard BLEU-4** uses n-grams from 1 to 4 with equal weights (0.25 each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate: str, references: list[str], max_n: int = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score with breakdown.\n",
    "    \"\"\"\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    \n",
    "    # Find closest reference length\n",
    "    ref_lens = [len(tokenize(ref)) for ref in references]\n",
    "    closest_ref_len = min(ref_lens, key=lambda r: abs(r - len(cand_tokens)))\n",
    "    \n",
    "    # Calculate precisions\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        p = modified_precision(candidate, references, n)\n",
    "        precisions.append(p)\n",
    "    \n",
    "    # Geometric mean (with smoothing for zeros)\n",
    "    epsilon = 1e-10\n",
    "    log_precisions = [math.log(max(p, epsilon)) for p in precisions]\n",
    "    geo_mean = math.exp(sum(log_precisions) / len(log_precisions))\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = brevity_penalty(len(cand_tokens), closest_ref_len)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bp * geo_mean,\n",
    "        \"brevity_penalty\": bp,\n",
    "        \"precisions\": precisions,\n",
    "        \"geometric_mean\": geo_mean,\n",
    "    }\n",
    "\n",
    "# Calculate BLEU for both candidates\n",
    "result1 = bleu_score(candidate1, references)\n",
    "result2 = bleu_score(candidate2, references)\n",
    "\n",
    "print(\"Candidate 1 (good translation):\")\n",
    "print(f\"  Precisions: {[f'{p:.3f}' for p in result1['precisions']]}\")\n",
    "print(f\"  BLEU: {result1['bleu']:.3f}\")\n",
    "\n",
    "print(\"\\nCandidate 2 (poor translation):\")\n",
    "print(f\"  Precisions: {[f'{p:.3f}' for p in result2['precisions']]}\")\n",
    "print(f\"  BLEU: {result2['bleu']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU scores reflect what we'd expect:\n",
    "- **Candidate 1** (the good translation) scores substantially higher\n",
    "- **Candidate 2** (the poor translation) is penalized for its bad n-gram precision at higher levels\n",
    "\n",
    "**Interpreting BLEU scores:**\n",
    "- BLEU is typically reported as a score between 0 and 1 (or 0-100 as a percentage)\n",
    "- Scores above 0.40 generally indicate good translations\n",
    "- Scores above 0.60 are excellent\n",
    "- Human translations typically score around 0.60-0.80 against each other (there's natural variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building ROUGE from Scratch\n",
    "\n",
    "While BLEU focuses on **precision** (how much of the candidate is correct), ROUGE focuses on **recall** (how much of the reference is covered).\n",
    "\n",
    "| Metric | Primary Question | Best For |\n",
    "|--------|------------------|----------|\n",
    "| BLEU | \"Is the output correct?\" | Machine Translation |\n",
    "| ROUGE | \"Is the reference covered?\" | Summarization |\n",
    "\n",
    "**Why recall for summarization?** A summary should capture the key information from the source. We care more about \"Did the summary include the important points?\" than \"Is every word in the summary justified?\"\n",
    "\n",
    "ROUGE computes:\n",
    "- **Recall:** What fraction of reference n-grams appear in the candidate?\n",
    "- **Precision:** What fraction of candidate n-grams appear in the reference?\n",
    "- **F1:** Harmonic mean of precision and recall\n",
    "\n",
    "Typically, ROUGE-1 recall (or ROUGE-L F1) is the primary metric reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(candidate: str, reference: str, n: int = 1) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-N recall, precision, and F1.\n",
    "    \"\"\"\n",
    "    cand_ngrams = Counter(get_ngrams(tokenize(candidate), n))\n",
    "    ref_ngrams = Counter(get_ngrams(tokenize(reference), n))\n",
    "    \n",
    "    # Count matches (clipped)\n",
    "    matches = sum(\n",
    "        min(cand_ngrams[ng], ref_ngrams[ng])\n",
    "        for ng in ref_ngrams\n",
    "    )\n",
    "    \n",
    "    recall = matches / sum(ref_ngrams.values()) if ref_ngrams else 0\n",
    "    precision = matches / sum(cand_ngrams.values()) if cand_ngrams else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\"recall\": recall, \"precision\": precision, \"f1\": f1}\n",
    "\n",
    "# Example from the book\n",
    "reference = \"Google announced new AI features for search.\"\n",
    "candidate = \"Google revealed AI search capabilities.\"\n",
    "\n",
    "r1 = rouge_n(candidate, reference, n=1)\n",
    "r2 = rouge_n(candidate, reference, n=2)\n",
    "\n",
    "print(f\"Reference: '{reference}'\")\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(f\"\\nROUGE-1: recall={r1['recall']:.3f}, precision={r1['precision']:.3f}, F1={r1['f1']:.3f}\")\n",
    "print(f\"ROUGE-2: recall={r2['recall']:.3f}, precision={r2['precision']:.3f}, F1={r2['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **ROUGE-1** (unigrams): Measures vocabulary overlap. The candidate captures about 43% of the reference's words.\n",
    "- **ROUGE-2** (bigrams): Measures phrase overlap. Only about 14% of reference bigrams appear in the candidate.\n",
    "\n",
    "The lower ROUGE-2 score indicates that while the candidate uses similar vocabulary, the phrasing is quite different from the reference. This is common with paraphrases.\n",
    "\n",
    "**Why F1?** In summarization, we often care about both precision and recall. F1 gives a balanced view:\n",
    "$$F1 = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ROUGE-L: Longest Common Subsequence\n",
    "\n",
    "ROUGE-N requires exact contiguous matches, but sometimes good text has the right words in roughly the right order without being exactly contiguous.\n",
    "\n",
    "**ROUGE-L** uses the **Longest Common Subsequence (LCS)** algorithm:\n",
    "- A subsequence doesn't need to be contiguous, just in the same order\n",
    "- Automatically rewards longer in-sequence matches\n",
    "- More flexible than strict n-gram matching\n",
    "\n",
    "**Example:**\n",
    "- Reference: \"The quick brown fox\"\n",
    "- Candidate: \"The brown quick fox\"\n",
    "- LCS: \"The\", \"brown\", \"fox\" (length 3) - \"quick\" is out of order\n",
    "\n",
    "The LCS algorithm uses dynamic programming to efficiently find the longest matching subsequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length(x: list, y: list) -> int:\n",
    "    \"\"\"Calculate length of longest common subsequence.\"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l(candidate: str, reference: str) -> dict:\n",
    "    \"\"\"Calculate ROUGE-L using longest common subsequence.\"\"\"\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    ref_tokens = tokenize(reference)\n",
    "    \n",
    "    lcs = lcs_length(ref_tokens, cand_tokens)\n",
    "    \n",
    "    recall = lcs / len(ref_tokens) if ref_tokens else 0\n",
    "    precision = lcs / len(cand_tokens) if cand_tokens else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\"lcs\": lcs, \"recall\": recall, \"precision\": precision, \"f1\": f1}\n",
    "\n",
    "# Example from the book\n",
    "reference = \"The company announced strong quarterly earnings\"\n",
    "candidate = \"Strong earnings were announced by the company\"\n",
    "\n",
    "result = rouge_l(candidate, reference)\n",
    "print(f\"Reference: '{reference}'\")\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(f\"\\nLCS length: {result['lcs']}\")\n",
    "print(f\"ROUGE-L recall: {result['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- The candidate is a paraphrase (passive voice, reordered)\n",
    "- ROUGE-L finds an LCS of 4 words that appear in both, in order\n",
    "- This gives a recall of 0.67 - the candidate captures most of the reference content despite different phrasing\n",
    "\n",
    "**When to use ROUGE-L vs ROUGE-N:**\n",
    "- **ROUGE-N:** When exact phrasing matters (news summaries, factual text)\n",
    "- **ROUGE-L:** When you want flexibility for paraphrases and different orderings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Using Production Implementations\n",
    "\n",
    "Our from-scratch implementations help us understand the algorithms, but in practice you should use established libraries like Hugging Face's `evaluate`.\n",
    "\n",
    "**Why use library implementations?**\n",
    "- Thoroughly tested and validated\n",
    "- Handle edge cases (empty strings, unicode, etc.)\n",
    "- Optimized for performance\n",
    "- Consistent with published results\n",
    "\n",
    "The `evaluate` library provides standardized implementations of BLEU, ROUGE, and many other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# BLEU expects: predictions (list of str), references (list of list of str)\n",
    "bleu_result = bleu.compute(\n",
    "    predictions=[candidate1],\n",
    "    references=[[ref1, ref2, ref3]]\n",
    ")\n",
    "\n",
    "# ROUGE expects: predictions, references (both list of str)\n",
    "rouge_result = rouge.compute(\n",
    "    predictions=[\"Google revealed AI search capabilities.\"],\n",
    "    references=[\"Google announced new AI features for search.\"]\n",
    ")\n",
    "\n",
    "print(\"BLEU (evaluate library):\")\n",
    "print(f\"  Score: {bleu_result['bleu']:.3f}\")\n",
    "print(f\"  Precisions: {[f'{p:.3f}' for p in bleu_result['precisions']]}\")\n",
    "\n",
    "print(\"\\nROUGE (evaluate library):\")\n",
    "for key, value in rouge_result.items():\n",
    "    print(f\"  {key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the API differences:**\n",
    "- **BLEU:** Expects `references` as a list of lists (each prediction can have multiple references)\n",
    "- **ROUGE:** Expects `references` as a list of strings (one reference per prediction)\n",
    "\n",
    "The library also provides additional variants like `rougeLsum` (sentence-level ROUGE-L for multi-sentence summaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Test your understanding of BLEU and ROUGE with these exercises.\n",
    "\n",
    "### Exercise 1: Brevity Penalty in Action\n",
    "\n",
    "What happens when we try to game BLEU with a very short but \"correct\" output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "result = bleu_score(\"party\", [\"The military follows party commands\"])\n",
    "print(f\"BLEU: {result['bleu']:.4f}\")\n",
    "print(f\"Brevity penalty crushes the score: {result['brevity_penalty']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: ROUGE-2 and Paraphrasing\n",
    "\n",
    "Why does ROUGE-2 often return 0 for short paraphrased texts?\n",
    "\n",
    "*Think about it:* If two sentences use the same words but in different order, what happens to bigram overlap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement ROUGE-S (Skip-bigram)\n",
    "\n",
    "ROUGE-S allows gaps between words in bigrams. Try implementing it for the ROUGE-L example sentence pair above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**N-grams** are the foundation of traditional text evaluation:\n",
    "- Unigrams capture vocabulary, higher n-grams capture phrase structure\n",
    "- Comparing n-gram overlap gives a quantitative similarity measure\n",
    "\n",
    "**BLEU** (Precision-oriented, for Machine Translation):\n",
    "- Modified precision clips counts to prevent gaming by repetition\n",
    "- Geometric mean ensures failure at any n-gram level tanks the score\n",
    "- Brevity penalty prevents gaming by outputting only confident words\n",
    "- Combine all n-gram levels (typically 1-4) for a holistic score\n",
    "\n",
    "**ROUGE** (Recall-oriented, for Summarization):\n",
    "- ROUGE-N measures recall of n-grams (how much reference content is covered)\n",
    "- ROUGE-L uses longest common subsequence for flexible matching\n",
    "- Reports precision, recall, and F1 (typically F1 is primary metric)\n",
    "\n",
    "### Strengths of These Metrics\n",
    "\n",
    "1. **Fast and deterministic** - No model inference needed\n",
    "2. **Interpretable** - Based on clear counting operations\n",
    "3. **Language-agnostic** - Work on any language with word boundaries\n",
    "4. **Well-established** - Decades of research and correlation studies\n",
    "\n",
    "### Limitations to Be Aware Of\n",
    "\n",
    "1. **Ignore semantics** - \"The dog bit the man\" vs \"The man bit the dog\" may score similarly\n",
    "2. **Require references** - Can't evaluate open-ended generation\n",
    "3. **Penalize valid paraphrases** - Correct but differently-worded text scores lower\n",
    "4. **Correlation varies by domain** - BLEU correlates well with human judgment for MT, less so for other tasks\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Task | Primary Metric | Why |\n",
    "|------|----------------|-----|\n",
    "| Machine Translation | BLEU | Precision matters; multiple references available |\n",
    "| Summarization | ROUGE-1/2/L | Recall matters; need to cover source content |\n",
    "| General Text Gen | Consider alternatives | BLEU/ROUGE may not capture quality well |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Chapter 3, we'll explore **semantic metrics** like BERTScore and COMET that use neural networks to compare meaning rather than exact word overlap. These address the key limitation of n-gram metrics: they can recognize that \"The automobile was repaired\" and \"The car was fixed\" are semantically equivalent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
