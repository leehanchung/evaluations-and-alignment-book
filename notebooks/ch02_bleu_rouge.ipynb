{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: BLEU and ROUGE\n",
    "\n",
    "Hands-on implementation of n-gram metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams from Scratch\n",
    "\n",
    "An n-gram is a contiguous sequence of n tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens: list[str], n: int) -> list[tuple]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Simple whitespace tokenizer with lowercasing.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "# Example from the book\n",
    "sentence = \"The cat is on the mat\"\n",
    "tokens = tokenize(sentence)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Unigrams (n=1): {get_ngrams(tokens, 1)}\")\n",
    "print(f\"Bigrams (n=2): {get_ngrams(tokens, 2)}\")\n",
    "print(f\"Trigrams (n=3): {get_ngrams(tokens, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaming Problem: Why Modified Precision?\n",
    "\n",
    "Naive precision can be gamed with repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 from the book\n",
    "candidate = \"the the the the the the the\"\n",
    "reference1 = \"The cat is on the mat.\"\n",
    "reference2 = \"There is a cat on the mat.\"\n",
    "\n",
    "cand_tokens = tokenize(candidate)\n",
    "ref1_tokens = tokenize(reference1)\n",
    "ref2_tokens = tokenize(reference2)\n",
    "\n",
    "# Naive precision: what fraction of candidate words appear in references?\n",
    "ref_words = set(ref1_tokens) | set(ref2_tokens)\n",
    "matches = sum(1 for w in cand_tokens if w in ref_words)\n",
    "naive_precision = matches / len(cand_tokens)\n",
    "\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(f\"Naive precision: {matches}/{len(cand_tokens)} = {naive_precision:.0%}\")\n",
    "print(\"\\n^ This is nonsense but scores 100%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified N-gram Precision\n",
    "\n",
    "BLEU clips each word count by its maximum occurrence in any reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate: str, references: list[str], n: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Calculate modified n-gram precision.\n",
    "    Clips counts by max occurrence in any single reference.\n",
    "    \"\"\"\n",
    "    cand_ngrams = get_ngrams(tokenize(candidate), n)\n",
    "    cand_counts = Counter(cand_ngrams)\n",
    "    \n",
    "    # Get max count for each n-gram across all references\n",
    "    max_ref_counts = Counter()\n",
    "    for ref in references:\n",
    "        ref_counts = Counter(get_ngrams(tokenize(ref), n))\n",
    "        for ngram, count in ref_counts.items():\n",
    "            max_ref_counts[ngram] = max(max_ref_counts[ngram], count)\n",
    "    \n",
    "    # Clip candidate counts\n",
    "    clipped_count = sum(\n",
    "        min(count, max_ref_counts.get(ngram, 0))\n",
    "        for ngram, count in cand_counts.items()\n",
    "    )\n",
    "    total_count = sum(cand_counts.values())\n",
    "    \n",
    "    return clipped_count / total_count if total_count > 0 else 0\n",
    "\n",
    "# Test on the gaming example\n",
    "references = [reference1, reference2]\n",
    "mod_precision = modified_precision(candidate, references, n=1)\n",
    "\n",
    "print(f\"Modified precision: {mod_precision:.2f}\")\n",
    "print(f\"'the' appears max 2 times in any reference\")\n",
    "print(f\"So: 2/7 = {2/7:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU on the Book's Translation Example\n",
    "\n",
    "Chinese: 「它是保证军队永远听党指挥的行动指南。」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 from the book\n",
    "candidate1 = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "candidate2 = \"It is to ensure the troops forever hearing the activity guidebook that party direct.\"\n",
    "\n",
    "ref1 = \"It is a guide to action that ensures that the military will forever heed Party commands.\"\n",
    "ref2 = \"It is the guiding principle which guarantees the military forces always being under the command of the Party.\"\n",
    "ref3 = \"It is the practical guide for the army always to heed the directions of the party.\"\n",
    "\n",
    "references = [ref1, ref2, ref3]\n",
    "\n",
    "# Calculate precision for different n-gram levels\n",
    "print(\"N-gram precisions:\")\n",
    "print(\"-\" * 40)\n",
    "for n in range(1, 5):\n",
    "    p1 = modified_precision(candidate1, references, n)\n",
    "    p2 = modified_precision(candidate2, references, n)\n",
    "    print(f\"{n}-gram: Candidate1={p1:.3f}, Candidate2={p2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric vs Arithmetic Mean\n",
    "\n",
    "Why geometric mean? It has the **zero-product property**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def geometric_mean(values):\n",
    "    product = 1\n",
    "    for v in values:\n",
    "        product *= v\n",
    "    return product ** (1 / len(values))\n",
    "\n",
    "# System A: Balanced performance\n",
    "system_a = [0.80, 0.60, 0.40, 0.20]\n",
    "\n",
    "# System B: Degenerate - only unigrams work\n",
    "system_b = [0.80, 0.00, 0.00, 0.00]\n",
    "\n",
    "print(\"System A (balanced): p1=0.80, p2=0.60, p3=0.40, p4=0.20\")\n",
    "print(f\"  Arithmetic mean: {arithmetic_mean(system_a):.3f}\")\n",
    "print(f\"  Geometric mean:  {geometric_mean(system_a):.3f}\")\n",
    "\n",
    "print(\"\\nSystem B (degenerate): p1=0.80, p2=0.00, p3=0.00, p4=0.00\")\n",
    "print(f\"  Arithmetic mean: {arithmetic_mean(system_b):.3f}\")\n",
    "print(f\"  Geometric mean:  {geometric_mean(system_b):.3f}\")\n",
    "print(\"\\n^ Geometric mean = 0 if ANY component fails!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brevity Penalty\n",
    "\n",
    "Prevents gaming by outputting only confident words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate_len: int, reference_len: int) -> float:\n",
    "    \"\"\"Calculate BLEU brevity penalty.\"\"\"\n",
    "    if candidate_len >= reference_len:\n",
    "        return 1.0\n",
    "    return math.exp(1 - reference_len / candidate_len)\n",
    "\n",
    "# Gaming example: very short candidate\n",
    "candidate_short = \"the military party\"  # 3 words\n",
    "reference = ref1  # 16 words\n",
    "\n",
    "c_len = len(tokenize(candidate_short))\n",
    "r_len = len(tokenize(reference))\n",
    "\n",
    "bp = brevity_penalty(c_len, r_len)\n",
    "precision = modified_precision(candidate_short, [reference], n=1)\n",
    "\n",
    "print(f\"Candidate: '{candidate_short}' ({c_len} words)\")\n",
    "print(f\"Reference length: {r_len} words\")\n",
    "print(f\"\\nUnigram precision: {precision:.2f} (looks perfect!)\")\n",
    "print(f\"Brevity penalty: {bp:.4f}\")\n",
    "print(f\"Adjusted score: {precision * bp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete BLEU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate: str, references: list[str], max_n: int = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score with breakdown.\n",
    "    \"\"\"\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    \n",
    "    # Find closest reference length\n",
    "    ref_lens = [len(tokenize(ref)) for ref in references]\n",
    "    closest_ref_len = min(ref_lens, key=lambda r: abs(r - len(cand_tokens)))\n",
    "    \n",
    "    # Calculate precisions\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        p = modified_precision(candidate, references, n)\n",
    "        precisions.append(p)\n",
    "    \n",
    "    # Geometric mean (with smoothing for zeros)\n",
    "    epsilon = 1e-10\n",
    "    log_precisions = [math.log(max(p, epsilon)) for p in precisions]\n",
    "    geo_mean = math.exp(sum(log_precisions) / len(log_precisions))\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = brevity_penalty(len(cand_tokens), closest_ref_len)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bp * geo_mean,\n",
    "        \"brevity_penalty\": bp,\n",
    "        \"precisions\": precisions,\n",
    "        \"geometric_mean\": geo_mean,\n",
    "    }\n",
    "\n",
    "# Calculate BLEU for both candidates\n",
    "result1 = bleu_score(candidate1, references)\n",
    "result2 = bleu_score(candidate2, references)\n",
    "\n",
    "print(\"Candidate 1 (good translation):\")\n",
    "print(f\"  Precisions: {[f'{p:.3f}' for p in result1['precisions']]}\")\n",
    "print(f\"  BLEU: {result1['bleu']:.3f}\")\n",
    "\n",
    "print(\"\\nCandidate 2 (poor translation):\")\n",
    "print(f\"  Precisions: {[f'{p:.3f}' for p in result2['precisions']]}\")\n",
    "print(f\"  BLEU: {result2['bleu']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE: Recall-Oriented Evaluation\n",
    "\n",
    "BLEU asks: \"What fraction of the candidate is correct?\" (precision)\n",
    "\n",
    "ROUGE asks: \"What fraction of the reference is covered?\" (recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(candidate: str, reference: str, n: int = 1) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-N recall, precision, and F1.\n",
    "    \"\"\"\n",
    "    cand_ngrams = Counter(get_ngrams(tokenize(candidate), n))\n",
    "    ref_ngrams = Counter(get_ngrams(tokenize(reference), n))\n",
    "    \n",
    "    # Count matches (clipped)\n",
    "    matches = sum(\n",
    "        min(cand_ngrams[ng], ref_ngrams[ng])\n",
    "        for ng in ref_ngrams\n",
    "    )\n",
    "    \n",
    "    recall = matches / sum(ref_ngrams.values()) if ref_ngrams else 0\n",
    "    precision = matches / sum(cand_ngrams.values()) if cand_ngrams else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\"recall\": recall, \"precision\": precision, \"f1\": f1}\n",
    "\n",
    "# Example from the book\n",
    "reference = \"Google announced new AI features for search.\"\n",
    "candidate = \"Google revealed AI search capabilities.\"\n",
    "\n",
    "r1 = rouge_n(candidate, reference, n=1)\n",
    "r2 = rouge_n(candidate, reference, n=2)\n",
    "\n",
    "print(f\"Reference: '{reference}'\")\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(f\"\\nROUGE-1: recall={r1['recall']:.3f}, precision={r1['precision']:.3f}, F1={r1['f1']:.3f}\")\n",
    "print(f\"ROUGE-2: recall={r2['recall']:.3f}, precision={r2['precision']:.3f}, F1={r2['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-L: Longest Common Subsequence\n",
    "\n",
    "Matches don't need to be contiguous—just in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_length(x: list, y: list) -> int:\n",
    "    \"\"\"Calculate length of longest common subsequence.\"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l(candidate: str, reference: str) -> dict:\n",
    "    \"\"\"Calculate ROUGE-L using longest common subsequence.\"\"\"\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    ref_tokens = tokenize(reference)\n",
    "    \n",
    "    lcs = lcs_length(ref_tokens, cand_tokens)\n",
    "    \n",
    "    recall = lcs / len(ref_tokens) if ref_tokens else 0\n",
    "    precision = lcs / len(cand_tokens) if cand_tokens else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\"lcs\": lcs, \"recall\": recall, \"precision\": precision, \"f1\": f1}\n",
    "\n",
    "# Example from the book\n",
    "reference = \"The company announced strong quarterly earnings\"\n",
    "candidate = \"Strong earnings were announced by the company\"\n",
    "\n",
    "result = rouge_l(candidate, reference)\n",
    "print(f\"Reference: '{reference}'\")\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(f\"\\nLCS length: {result['lcs']}\")\n",
    "print(f\"ROUGE-L recall: {result['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `evaluate` Library\n",
    "\n",
    "In practice, use established implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# BLEU expects: predictions (list of str), references (list of list of str)\n",
    "bleu_result = bleu.compute(\n",
    "    predictions=[candidate1],\n",
    "    references=[[ref1, ref2, ref3]]\n",
    ")\n",
    "\n",
    "# ROUGE expects: predictions, references (both list of str)\n",
    "rouge_result = rouge.compute(\n",
    "    predictions=[\"Google revealed AI search capabilities.\"],\n",
    "    references=[\"Google announced new AI features for search.\"]\n",
    ")\n",
    "\n",
    "print(\"BLEU (evaluate library):\")\n",
    "print(f\"  Score: {bleu_result['bleu']:.3f}\")\n",
    "print(f\"  Precisions: {[f'{p:.3f}' for p in bleu_result['precisions']]}\")\n",
    "\n",
    "print(\"\\nROUGE (evaluate library):\")\n",
    "for key, value in rouge_result.items():\n",
    "    print(f\"  {key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Calculate BLEU for this gaming attempt: Candidate=\"party\" vs Reference=\"The military follows party commands\"\n",
    "\n",
    "2. Why does ROUGE-2 often return 0 for short paraphrased texts?\n",
    "\n",
    "3. Implement ROUGE-S (skip-bigram) for the sentence pair in the ROUGE-L example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "result = bleu_score(\"party\", [\"The military follows party commands\"])\n",
    "print(f\"BLEU: {result['bleu']:.4f}\")\n",
    "print(f\"Brevity penalty crushes the score: {result['brevity_penalty']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
