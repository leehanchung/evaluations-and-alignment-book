{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 4: LLM-as-a-Judge\n\nHands-on implementation of LLM-based evaluation systems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom scipy.stats import spearmanr, kendalltau\nfrom sklearn.metrics import cohen_kappa_score"
  },
  {
   "cell_type": "markdown",
   "source": "## Measuring Evaluator Agreement\n\nBefore trusting any evaluator (human or LLM), we need statistical tools to measure agreement.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example from the book: comparing human scores with metric scores\nhuman_scores = [4, 2, 5, 3, 1, 4, 3, 5, 2, 1]\nmetric_scores = [0.7, 0.3, 0.9, 0.5, 0.1, 0.8, 0.4, 0.85, 0.25, 0.15]\n\n# Spearman's rank correlation\ncorr, p_value = spearmanr(human_scores, metric_scores)\nprint(f\"Spearman's ρ: {corr:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Bootstrap confidence intervals\nn_bootstraps = 1000\nbootstrap_corrs = []\nfor _ in range(n_bootstraps):\n    indices = np.random.choice(len(human_scores), len(human_scores), replace=True)\n    resampled_human = [human_scores[i] for i in indices]\n    resampled_metric = [metric_scores[i] for i in indices]\n    boot_corr, _ = spearmanr(resampled_human, resampled_metric)\n    bootstrap_corrs.append(boot_corr)\n\nlower = np.percentile(bootstrap_corrs, 2.5)\nupper = np.percentile(bootstrap_corrs, 97.5)\nprint(f\"95% CI: [{lower:.3f}, {upper:.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Kendall's Tau: Pairwise Agreement\n\nKendall's τ counts concordant vs discordant pairs. A τ of 0.8 means 80% of pairs agree on ordering.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example from the book\nhuman_ranks = [1, 2, 3, 4, 5]\nmetric_ranks = [1, 3, 2, 5, 4]\n\ntau, p_value = kendalltau(human_ranks, metric_ranks)\nprint(f\"Kendall's τ: {tau:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Count concordant and discordant pairs manually\nn = len(human_ranks)\nconcordant = discordant = 0\nfor i in range(n):\n    for j in range(i + 1, n):\n        h_diff = human_ranks[i] - human_ranks[j]\n        m_diff = metric_ranks[i] - metric_ranks[j]\n        if h_diff * m_diff > 0:\n            concordant += 1\n        elif h_diff * m_diff < 0:\n            discordant += 1\n\nprint(f\"\\nConcordant pairs: {concordant}\")\nprint(f\"Discordant pairs: {discordant}\")\nprint(f\"Manual τ: {(concordant - discordant) / (concordant + discordant):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cohen's Kappa: Categorical Agreement\n\nFor binary/categorical judgments (pass/fail, safe/unsafe), Cohen's κ corrects for chance agreement.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Two raters labeling responses as acceptable (1) or not (0)\nrater1 = [1, 1, 0, 1, 1, 0, 1, 0, 1, 1]\nrater2 = [1, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n\nkappa = cohen_kappa_score(rater1, rater2)\n\n# Compute manually to understand\nobserved_agreement = sum(r1 == r2 for r1, r2 in zip(rater1, rater2)) / len(rater1)\n\n# Expected agreement by chance\np1_yes = sum(rater1) / len(rater1)\np2_yes = sum(rater2) / len(rater2)\nexpected_agreement = p1_yes * p2_yes + (1 - p1_yes) * (1 - p2_yes)\n\nmanual_kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n\nprint(f\"Observed agreement: {observed_agreement:.0%}\")\nprint(f\"Expected (chance) agreement: {expected_agreement:.0%}\")\nprint(f\"Cohen's κ: {kappa:.3f}\")\nprint(f\"\\nInterpretation (Landis & Koch):\")\nprint(\"  0.81-1.00: Almost perfect\")\nprint(\"  0.61-0.80: Substantial\")\nprint(\"  0.41-0.60: Moderate\")\nprint(\"  0.21-0.40: Fair\")\nprint(\"  0.00-0.20: Slight\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## G-Eval: Systematic LLM-based Evaluation\n\nG-Eval's three components: structured prompts, auto-generated CoT, and probability-weighted scoring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# G-Eval prompt template for coherence evaluation\nGEVAL_COHERENCE_PROMPT = \"\"\"You will be given one summary written for a news article.\n\nYour task is to rate the summary on one metric.\n\nPlease make sure you read and understand these instructions carefully.\n\nEvaluation Criteria:\n\nCoherence (1-5) - the collective quality of all sentences. The summary should \nbe well-structured and well-organized. The summary should not just be a heap \nof related information, but should build from sentence to sentence into a \ncoherent body of information about a topic.\n\nEvaluation Steps:\n\n1. Read the news article carefully and identify the main topic and key points.\n2. Read the summary and compare it to the news article. Check if the summary \n   covers the main topic and key points, and if it presents them in a clear \n   and logical order.\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest \n   and 5 is the highest, based on the Evaluation Criteria.\n\nSource Text:\n{document}\n\nSummary:\n{summary}\n\nEvaluation Form (scores ONLY):\n- Coherence: \"\"\"\n\n# Example usage\ndocument = \"\"\"The quarterly earnings report showed a 15% increase in revenue, \ndriven primarily by strong performance in the cloud services division. \nHowever, operating costs rose by 8%, partially offsetting gains.\"\"\"\n\nsummary = \"\"\"The company's revenue grew substantially due to cloud services' \nsuccess, though higher operating expenses moderated the overall financial \nimprovement.\"\"\"\n\nprint(\"G-Eval Coherence Prompt:\")\nprint(\"-\" * 50)\nprint(GEVAL_COHERENCE_PROMPT.format(document=document, summary=summary))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Probability-Weighted Scoring\n\nInstead of discrete scores, use token probabilities to compute expected values.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def probability_weighted_score(score_probs: dict[int, float]) -> float:\n    \"\"\"\n    Compute expected score from probability distribution.\n    \n    score_probs: {score: probability} e.g., {1: 0.1, 2: 0.2, 3: 0.5, 4: 0.15, 5: 0.05}\n    \"\"\"\n    return sum(score * prob for score, prob in score_probs.items())\n\n# Example from the book: two summaries with same discrete score but different distributions\nsummary_a_probs = {1: 0.0, 2: 0.05, 3: 0.70, 4: 0.20, 5: 0.05}  # Confident 3\nsummary_b_probs = {1: 0.0, 2: 0.35, 3: 0.55, 4: 0.08, 5: 0.02}  # Uncertain 2-3\n\nscore_a = probability_weighted_score(summary_a_probs)\nscore_b = probability_weighted_score(summary_b_probs)\n\nprint(\"Both would receive discrete score of 3, but:\")\nprint(f\"  Summary A weighted score: {score_a:.2f}\")\nprint(f\"  Summary B weighted score: {score_b:.2f}\")\nprint(f\"\\n^ Probability weighting correctly ranks A > B\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Implementing LLM-as-a-Judge\n\nUsing OpenAI's API with logprobs for probability-weighted scoring.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from openai import OpenAI\n\nclient = OpenAI()\n\ndef geval_score(document: str, summary: str, model: str = \"gpt-4o-mini\") -> dict:\n    \"\"\"\n    G-Eval style scoring with probability weighting.\n    Returns discrete score and weighted score.\n    \"\"\"\n    prompt = GEVAL_COHERENCE_PROMPT.format(document=document, summary=summary)\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=1,\n        logprobs=True,\n        top_logprobs=5\n    )\n    \n    # Extract token and logprobs\n    choice = response.choices[0]\n    discrete_score = int(choice.message.content.strip())\n    \n    # Build probability distribution over valid scores\n    score_probs = {i: 0.0 for i in range(1, 6)}\n    if choice.logprobs and choice.logprobs.content:\n        for item in choice.logprobs.content[0].top_logprobs:\n            token = item.token.strip()\n            if token in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n                score_probs[int(token)] = np.exp(item.logprob)\n    \n    # Normalize probabilities\n    total = sum(score_probs.values())\n    if total > 0:\n        score_probs = {k: v / total for k, v in score_probs.items()}\n    \n    weighted_score = probability_weighted_score(score_probs)\n    \n    return {\n        \"discrete_score\": discrete_score,\n        \"weighted_score\": weighted_score,\n        \"probabilities\": score_probs\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test G-Eval scoring\nresult = geval_score(document, summary)\n\nprint(f\"Document: {document[:60]}...\")\nprint(f\"Summary: {summary[:60]}...\")\nprint(f\"\\nDiscrete score: {result['discrete_score']}\")\nprint(f\"Weighted score: {result['weighted_score']:.2f}\")\nprint(f\"\\nProbability distribution:\")\nfor score, prob in result['probabilities'].items():\n    bar = \"█\" * int(prob * 20)\n    print(f\"  {score}: {prob:.2%} {bar}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Pairwise Comparison\n\nFor higher correlation with human preferences, compare two responses directly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "PAIRWISE_PROMPT = \"\"\"Please act as an impartial judge and evaluate the quality of the \nresponses provided by two AI assistants to the user's question.\n\nYour evaluation should consider correctness, helpfulness, and relevance.\n\nAvoid any position biases and ensure that the order in which the responses \nwere presented does not influence your decision.\n\n[User Question]\n{question}\n\n[Assistant A's Answer]\n{answer_a}\n\n[Assistant B's Answer]\n{answer_b}\n\nAfter providing your explanation, output your final verdict by strictly \nfollowing this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if \nassistant B is better, and \"[[C]]\" for a tie.\"\"\"\n\ndef pairwise_judge(\n    question: str, \n    answer_a: str, \n    answer_b: str,\n    model: str = \"gpt-4o-mini\"\n) -> dict:\n    \"\"\"Judge which response is better using pairwise comparison.\"\"\"\n    prompt = PAIRWISE_PROMPT.format(\n        question=question,\n        answer_a=answer_a,\n        answer_b=answer_b\n    )\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=500\n    )\n    \n    content = response.choices[0].message.content\n    \n    # Extract verdict\n    if \"[[A]]\" in content:\n        verdict = \"A\"\n    elif \"[[B]]\" in content:\n        verdict = \"B\"\n    elif \"[[C]]\" in content:\n        verdict = \"tie\"\n    else:\n        verdict = \"unknown\"\n    \n    return {\"verdict\": verdict, \"reasoning\": content}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test pairwise comparison\nquestion = \"What is the capital of France?\"\nanswer_a = \"The capital of France is Paris.\"\nanswer_b = \"Paris is the capital city of France, known for the Eiffel Tower and rich cultural heritage.\"\n\nresult = pairwise_judge(question, answer_a, answer_b)\nprint(f\"Question: {question}\")\nprint(f\"\\nAssistant A: {answer_a}\")\nprint(f\"Assistant B: {answer_b}\")\nprint(f\"\\nVerdict: {result['verdict']}\")\nprint(f\"\\nReasoning:\\n{result['reasoning']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Mitigating Position Bias\n\nRun comparisons twice with swapped positions to detect and correct for position bias.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def pairwise_judge_debiased(\n    question: str,\n    answer_a: str,\n    answer_b: str,\n    model: str = \"gpt-4o-mini\"\n) -> dict:\n    \"\"\"\n    Run pairwise comparison twice with swapped positions.\n    Only return a verdict if both runs agree.\n    \"\"\"\n    # Original order: A first, B second\n    result1 = pairwise_judge(question, answer_a, answer_b, model)\n    \n    # Swapped order: B first, A second\n    result2 = pairwise_judge(question, answer_b, answer_a, model)\n    \n    # Map swapped result back\n    swapped_verdict = {\"A\": \"B\", \"B\": \"A\", \"tie\": \"tie\"}.get(result2[\"verdict\"], \"unknown\")\n    \n    # Check agreement\n    if result1[\"verdict\"] == swapped_verdict:\n        confident = True\n        final_verdict = result1[\"verdict\"]\n    else:\n        confident = False\n        final_verdict = \"inconclusive\"\n    \n    return {\n        \"verdict\": final_verdict,\n        \"confident\": confident,\n        \"original_order\": result1[\"verdict\"],\n        \"swapped_order\": result2[\"verdict\"]\n    }\n\n# Test debiased comparison\nresult = pairwise_judge_debiased(question, answer_a, answer_b)\nprint(f\"Original order verdict: {result['original_order']}\")\nprint(f\"Swapped order verdict: {result['swapped_order']} (mapped back)\")\nprint(f\"Final verdict: {result['verdict']}\")\nprint(f\"Confident: {result['confident']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Reference-Guided Grading\n\nFor math/reasoning tasks, generate the reference answer separately to avoid context contamination.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "REFERENCE_GUIDED_PROMPT = \"\"\"Please act as an impartial judge and evaluate the quality \nof the responses provided by two AI assistants to the user's question.\n\nYour evaluation should consider correctness and helpfulness. You will be given \na reference answer, assistant A's answer, and assistant B's answer.\n\nYour job is to evaluate which assistant's answer is better.\n\nBegin your evaluation by comparing both assistants' answers with the reference \nanswer. Identify and correct any mistakes.\n\n[User Question]\n{question}\n\n[Reference Answer]\n{reference}\n\n[Assistant A's Answer]\n{answer_a}\n\n[Assistant B's Answer]\n{answer_b}\n\nAfter providing your explanation, output your final verdict: \"[[A]]\" if \nassistant A is better, \"[[B]]\" if assistant B is better, \"[[C]]\" for a tie.\"\"\"\n\ndef reference_guided_judge(\n    question: str,\n    answer_a: str,\n    answer_b: str,\n    model: str = \"gpt-4o-mini\"\n) -> dict:\n    \"\"\"\n    Two-phase evaluation:\n    1. Generate reference answer in clean context\n    2. Compare candidates against reference\n    \"\"\"\n    # Phase 1: Generate reference answer\n    ref_response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}],\n        max_tokens=500\n    )\n    reference = ref_response.choices[0].message.content\n    \n    # Phase 2: Compare with reference\n    prompt = REFERENCE_GUIDED_PROMPT.format(\n        question=question,\n        reference=reference,\n        answer_a=answer_a,\n        answer_b=answer_b\n    )\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=500\n    )\n    \n    content = response.choices[0].message.content\n    \n    if \"[[A]]\" in content:\n        verdict = \"A\"\n    elif \"[[B]]\" in content:\n        verdict = \"B\"\n    elif \"[[C]]\" in content:\n        verdict = \"tie\"\n    else:\n        verdict = \"unknown\"\n    \n    return {\n        \"verdict\": verdict,\n        \"reference\": reference,\n        \"reasoning\": content\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test reference-guided grading on a math problem\nmath_question = \"What is 15% of 80?\"\nanswer_correct = \"15% of 80 is 12.\"\nanswer_wrong = \"15% of 80 is 15.\"  # Common mistake\n\nresult = reference_guided_judge(math_question, answer_correct, answer_wrong)\nprint(f\"Question: {math_question}\")\nprint(f\"\\nAssistant A: {answer_correct}\")\nprint(f\"Assistant B: {answer_wrong}\")\nprint(f\"\\nReference: {result['reference']}\")\nprint(f\"\\nVerdict: {result['verdict']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Structured Output for Evaluation\n\nUse JSON schemas to ensure consistent, parseable outputs. Note: reasoning must come before score.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pydantic import BaseModel\n\nclass EvaluationResult(BaseModel):\n    \"\"\"Schema for structured evaluation output.\"\"\"\n    reasoning_steps: list[str]  # Must come BEFORE score (autoregressive ordering)\n    score: int\n\nSTRUCTURED_PROMPT = \"\"\"Evaluate this summary for coherence on a scale of 1-5.\n\nCoherence: The summary should be well-structured, well-organized, and build \nfrom sentence to sentence into a coherent body of information.\n\nDocument: {document}\nSummary: {summary}\n\nProvide step-by-step reasoning, then assign a score.\"\"\"\n\ndef structured_eval(document: str, summary: str, model: str = \"gpt-4o-mini\") -> dict:\n    \"\"\"Structured evaluation with reasoning before score.\"\"\"\n    response = client.beta.chat.completions.parse(\n        model=model,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": STRUCTURED_PROMPT.format(document=document, summary=summary)\n        }],\n        response_format=EvaluationResult\n    )\n    \n    result = response.choices[0].message.parsed\n    return {\n        \"reasoning\": result.reasoning_steps,\n        \"score\": result.score\n    }\n\n# Test structured evaluation\nresult = structured_eval(document, summary)\nprint(\"Reasoning steps:\")\nfor i, step in enumerate(result[\"reasoning\"], 1):\n    print(f\"  {i}. {step}\")\nprint(f\"\\nScore: {result['score']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Using Anthropic's Claude as a Judge\n\nClaude can also serve as an LLM judge with similar prompting patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from anthropic import Anthropic\n\nanthropic_client = Anthropic()\n\ndef claude_judge(\n    question: str,\n    answer_a: str,\n    answer_b: str,\n    model: str = \"claude-sonnet-4-20250514\"\n) -> dict:\n    \"\"\"Pairwise comparison using Claude.\"\"\"\n    prompt = PAIRWISE_PROMPT.format(\n        question=question,\n        answer_a=answer_a,\n        answer_b=answer_b\n    )\n    \n    response = anthropic_client.messages.create(\n        model=model,\n        max_tokens=500,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    content = response.content[0].text\n    \n    if \"[[A]]\" in content:\n        verdict = \"A\"\n    elif \"[[B]]\" in content:\n        verdict = \"B\"\n    elif \"[[C]]\" in content:\n        verdict = \"tie\"\n    else:\n        verdict = \"unknown\"\n    \n    return {\"verdict\": verdict, \"reasoning\": content}\n\n# Test Claude as judge\nresult = claude_judge(question, answer_a, answer_b)\nprint(f\"Claude's verdict: {result['verdict']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Exercises\n\n1. Compute Spearman's ρ and Kendall's τ between two human annotators' scores. When do they disagree?\n\n2. Implement a verbosity bias test: create two responses with identical content but different lengths. Does the judge prefer the longer one?\n\n3. Build a multi-dimensional rubric that evaluates responses on helpfulness, accuracy, and tone. Use geometric mean to combine scores.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exercise 2: Verbosity bias test\nconcise_answer = \"The capital of France is Paris.\"\nverbose_answer = \"\"\"The capital of France is Paris. Paris is a beautiful city \nlocated in the north-central part of France. It is known for many famous \nlandmarks including the Eiffel Tower, the Louvre Museum, and Notre-Dame \nCathedral. The city has been the capital since the 10th century and remains \nthe political, economic, and cultural center of France today.\"\"\"\n\nresult = pairwise_judge(\n    \"What is the capital of France?\",\n    concise_answer,\n    verbose_answer\n)\nprint(f\"Concise: {concise_answer}\")\nprint(f\"Verbose: {verbose_answer[:50]}...\")\nprint(f\"\\nVerdict: {result['verdict']}\")\nprint(\"^ Does the judge exhibit verbosity bias?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}