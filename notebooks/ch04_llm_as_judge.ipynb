{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: LLM-as-a-Judge\n",
    "\n",
    "Hands-on implementation of LLM-based evaluation systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "1. **Understand why LLM judges are powerful** - Learn when and why to use LLMs for evaluation instead of traditional metrics\n",
    "2. **Measure evaluator agreement** - Apply statistical tools (Spearman's rho, Kendall's tau, Cohen's kappa) to validate evaluation quality\n",
    "3. **Implement different judging approaches** - Build pointwise, pairwise, and reference-guided evaluation systems\n",
    "4. **Design effective judge prompts** - Create structured prompts with clear criteria and evaluation steps\n",
    "5. **Mitigate known biases** - Identify and correct for position bias, verbosity bias, and self-enhancement bias\n",
    "6. **Use probability-weighted scoring** - Extract richer signals from token probabilities instead of discrete scores\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use LLMs as Judges?\n",
    "\n",
    "Traditional evaluation metrics like BLEU, ROUGE, and BERTScore measure surface-level similarity between generated text and reference text. However, they struggle with:\n",
    "\n",
    "- **Open-ended tasks** where multiple valid responses exist\n",
    "- **Subjective qualities** like helpfulness, tone, and engagement\n",
    "- **Complex reasoning** that requires understanding, not just matching\n",
    "\n",
    "LLM-as-a-Judge offers several advantages:\n",
    "\n",
    "| Aspect | Traditional Metrics | LLM Judges |\n",
    "|--------|---------------------|------------|\n",
    "| Reference required | Yes | Optional |\n",
    "| Subjective evaluation | Limited | Strong |\n",
    "| Reasoning about content | Surface-level | Deep understanding |\n",
    "| Cost per evaluation | Very low | Moderate |\n",
    "| Consistency | Perfect | Variable (needs calibration) |\n",
    "\n",
    "The key insight is that **LLMs can approximate human judgment** at a fraction of the cost, enabling evaluation at scale for tasks where human annotation would be prohibitively expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Measuring Evaluator Agreement\n",
    "\n",
    "Before trusting any evaluator (human or LLM), we need statistical tools to measure how well it agrees with ground truth or other evaluators. This section introduces three essential metrics:\n",
    "\n",
    "1. **Spearman's rho** - For ranked/ordinal data\n",
    "2. **Kendall's tau** - For pairwise ranking agreement  \n",
    "3. **Cohen's kappa** - For categorical judgments\n",
    "\n",
    "### Setup\n",
    "\n",
    "We begin by importing the statistical libraries we need for measuring agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's Rank Correlation\n",
    "\n",
    "**Spearman's rho** measures monotonic relationships between two sets of rankings. Unlike Pearson correlation, it does not assume linearity - only that when one variable increases, the other tends to increase (or decrease) consistently.\n",
    "\n",
    "**Key properties:**\n",
    "- Range: -1 to +1 (1 = perfect agreement, -1 = perfect disagreement, 0 = no correlation)\n",
    "- Robust to outliers since it uses ranks, not raw values\n",
    "- Ideal for ordinal scales like 1-5 quality ratings\n",
    "\n",
    "**Why bootstrap confidence intervals?** A single correlation value tells us the point estimate, but bootstrap resampling shows us the uncertainty around that estimate. This is crucial when working with small sample sizes typical in evaluation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the book: comparing human scores with metric scores\n",
    "human_scores = [4, 2, 5, 3, 1, 4, 3, 5, 2, 1]\n",
    "metric_scores = [0.7, 0.3, 0.9, 0.5, 0.1, 0.8, 0.4, 0.85, 0.25, 0.15]\n",
    "\n",
    "# Spearman's rank correlation\n",
    "corr, p_value = spearmanr(human_scores, metric_scores)\n",
    "print(f\"Spearman's ρ: {corr:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "n_bootstraps = 1000\n",
    "bootstrap_corrs = []\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(len(human_scores), len(human_scores), replace=True)\n",
    "    resampled_human = [human_scores[i] for i in indices]\n",
    "    resampled_metric = [metric_scores[i] for i in indices]\n",
    "    boot_corr, _ = spearmanr(resampled_human, resampled_metric)\n",
    "    bootstrap_corrs.append(boot_corr)\n",
    "\n",
    "lower = np.percentile(bootstrap_corrs, 2.5)\n",
    "upper = np.percentile(bootstrap_corrs, 97.5)\n",
    "print(f\"95% CI: [{lower:.3f}, {upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall's Tau: Pairwise Agreement\n",
    "\n",
    "**Kendall's tau** takes a different approach: it counts concordant vs. discordant pairs. For any two items, if both evaluators agree on which is better, that is a concordant pair; otherwise, it is discordant.\n",
    "\n",
    "**Formula:**\n",
    "$$\\tau = \\frac{\\text{concordant pairs} - \\text{discordant pairs}}{\\text{total pairs}}$$\n",
    "\n",
    "**Why use Kendall's tau?**\n",
    "- A tau of 0.8 means 80% of pairs agree on ordering - this is directly interpretable\n",
    "- More intuitive for pairwise comparison tasks (like ranking models)\n",
    "- Better statistical properties for small samples than Spearman's rho\n",
    "\n",
    "**When to use each:**\n",
    "- Use Spearman's rho when you care about overall ranking correlation\n",
    "- Use Kendall's tau when pairwise preferences matter (e.g., \"Is model A better than model B?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the book\n",
    "human_ranks = [1, 2, 3, 4, 5]\n",
    "metric_ranks = [1, 3, 2, 5, 4]\n",
    "\n",
    "tau, p_value = kendalltau(human_ranks, metric_ranks)\n",
    "print(f\"Kendall's τ: {tau:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Count concordant and discordant pairs manually\n",
    "n = len(human_ranks)\n",
    "concordant = discordant = 0\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        h_diff = human_ranks[i] - human_ranks[j]\n",
    "        m_diff = metric_ranks[i] - metric_ranks[j]\n",
    "        if h_diff * m_diff > 0:\n",
    "            concordant += 1\n",
    "        elif h_diff * m_diff < 0:\n",
    "            discordant += 1\n",
    "\n",
    "print(f\"\\nConcordant pairs: {concordant}\")\n",
    "print(f\"Discordant pairs: {discordant}\")\n",
    "print(f\"Manual τ: {(concordant - discordant) / (concordant + discordant):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa: Categorical Agreement\n",
    "\n",
    "For **binary or categorical judgments** (pass/fail, safe/unsafe, acceptable/unacceptable), simple percent agreement is misleading because two random raters would agree some percentage of the time just by chance.\n",
    "\n",
    "**Cohen's kappa** corrects for this:\n",
    "\n",
    "$$\\kappa = \\frac{P_o - P_e}{1 - P_e}$$\n",
    "\n",
    "Where:\n",
    "- $P_o$ = observed agreement\n",
    "- $P_e$ = expected agreement by chance\n",
    "\n",
    "**Interpretation guidelines (Landis & Koch, 1977):**\n",
    "| Kappa | Interpretation |\n",
    "|-------|---------------|\n",
    "| 0.81-1.00 | Almost perfect |\n",
    "| 0.61-0.80 | Substantial |\n",
    "| 0.41-0.60 | Moderate |\n",
    "| 0.21-0.40 | Fair |\n",
    "| 0.00-0.20 | Slight |\n",
    "\n",
    "**Why kappa matters for LLM judges:** When validating an LLM judge against human labels, kappa tells you how much better than random the LLM is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Two raters labeling responses as acceptable (1) or not (0)\n",
    "rater1 = [1, 1, 0, 1, 1, 0, 1, 0, 1, 1]\n",
    "rater2 = [1, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
    "\n",
    "kappa = cohen_kappa_score(rater1, rater2)\n",
    "\n",
    "# Compute manually to understand\n",
    "observed_agreement = sum(r1 == r2 for r1, r2 in zip(rater1, rater2)) / len(rater1)\n",
    "\n",
    "# Expected agreement by chance\n",
    "p1_yes = sum(rater1) / len(rater1)\n",
    "p2_yes = sum(rater2) / len(rater2)\n",
    "expected_agreement = p1_yes * p2_yes + (1 - p1_yes) * (1 - p2_yes)\n",
    "\n",
    "manual_kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n",
    "\n",
    "print(f\"Observed agreement: {observed_agreement:.0%}\")\n",
    "print(f\"Expected (chance) agreement: {expected_agreement:.0%}\")\n",
    "print(f\"Cohen's κ: {kappa:.3f}\")\n",
    "print(f\"\\nInterpretation (Landis & Koch):\")\n",
    "print(\"  0.81-1.00: Almost perfect\")\n",
    "print(\"  0.61-0.80: Substantial\")\n",
    "print(\"  0.41-0.60: Moderate\")\n",
    "print(\"  0.21-0.40: Fair\")\n",
    "print(\"  0.00-0.20: Slight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: G-Eval - Systematic LLM-based Evaluation\n",
    "\n",
    "[G-Eval](https://arxiv.org/abs/2303.16634) (Liu et al., 2023) is a framework for LLM-based evaluation that achieves high correlation with human judgments. It has three key components:\n",
    "\n",
    "1. **Structured evaluation prompts** - Clear criteria definition and evaluation steps\n",
    "2. **Auto-generated Chain-of-Thought** - LLM reasons through the evaluation before scoring\n",
    "3. **Probability-weighted scoring** - Uses token probabilities for finer-grained scores\n",
    "\n",
    "### Designing Effective Judge Prompts\n",
    "\n",
    "A good evaluation prompt should include:\n",
    "\n",
    "| Component | Purpose | Example |\n",
    "|-----------|---------|---------|\n",
    "| **Role definition** | Set expectations | \"You will be given one summary...\" |\n",
    "| **Metric definition** | Precise criteria | \"Coherence (1-5) - the collective quality of all sentences...\" |\n",
    "| **Evaluation steps** | Guide reasoning | \"1. Read the article... 2. Compare to summary...\" |\n",
    "| **Input format** | Structured data | \"Source Text: {document}\" |\n",
    "| **Output format** | Parseable result | \"Evaluation Form (scores ONLY):\" |\n",
    "\n",
    "The following prompt demonstrates these principles for coherence evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G-Eval prompt template for coherence evaluation\n",
    "GEVAL_COHERENCE_PROMPT = \"\"\"You will be given one summary written for a news article.\n",
    "\n",
    "Your task is to rate the summary on one metric.\n",
    "\n",
    "Please make sure you read and understand these instructions carefully.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Coherence (1-5) - the collective quality of all sentences. The summary should \n",
    "be well-structured and well-organized. The summary should not just be a heap \n",
    "of related information, but should build from sentence to sentence into a \n",
    "coherent body of information about a topic.\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "1. Read the news article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the news article. Check if the summary \n",
    "   covers the main topic and key points, and if it presents them in a clear \n",
    "   and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest \n",
    "   and 5 is the highest, based on the Evaluation Criteria.\n",
    "\n",
    "Source Text:\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "- Coherence: \"\"\"\n",
    "\n",
    "# Example usage\n",
    "document = \"\"\"The quarterly earnings report showed a 15% increase in revenue, \n",
    "driven primarily by strong performance in the cloud services division. \n",
    "However, operating costs rose by 8%, partially offsetting gains.\"\"\"\n",
    "\n",
    "summary = \"\"\"The company's revenue grew substantially due to cloud services' \n",
    "success, though higher operating expenses moderated the overall financial \n",
    "improvement.\"\"\"\n",
    "\n",
    "print(\"G-Eval Coherence Prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(GEVAL_COHERENCE_PROMPT.format(document=document, summary=summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability-Weighted Scoring\n",
    "\n",
    "A key insight from G-Eval is that **discrete scores lose information**. When an LLM outputs \"3\", it might be 90% confident in 3 and 10% in 4, or it might be 50% for 3 and 40% for 2. These represent different underlying assessments.\n",
    "\n",
    "**Probability-weighted scoring** extracts the LLM's full probability distribution over possible scores and computes the expected value:\n",
    "\n",
    "$$\\text{score} = \\sum_{i=1}^{5} i \\cdot P(i)$$\n",
    "\n",
    "**Benefits:**\n",
    "- **Finer granularity** - Can distinguish between items that would receive the same discrete score\n",
    "- **Better correlation** - Often achieves higher correlation with human judgments\n",
    "- **Uncertainty quantification** - High entropy in the distribution indicates uncertainty\n",
    "\n",
    "The following function computes expected scores from probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_weighted_score(score_probs: dict[int, float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute expected score from probability distribution.\n",
    "    \n",
    "    score_probs: {score: probability} e.g., {1: 0.1, 2: 0.2, 3: 0.5, 4: 0.15, 5: 0.05}\n",
    "    \"\"\"\n",
    "    return sum(score * prob for score, prob in score_probs.items())\n",
    "\n",
    "# Example from the book: two summaries with same discrete score but different distributions\n",
    "summary_a_probs = {1: 0.0, 2: 0.05, 3: 0.70, 4: 0.20, 5: 0.05}  # Confident 3\n",
    "summary_b_probs = {1: 0.0, 2: 0.35, 3: 0.55, 4: 0.08, 5: 0.02}  # Uncertain 2-3\n",
    "\n",
    "score_a = probability_weighted_score(summary_a_probs)\n",
    "score_b = probability_weighted_score(summary_b_probs)\n",
    "\n",
    "print(\"Both would receive discrete score of 3, but:\")\n",
    "print(f\"  Summary A weighted score: {score_a:.2f}\")\n",
    "print(f\"  Summary B weighted score: {score_b:.2f}\")\n",
    "print(f\"\\n^ Probability weighting correctly ranks A > B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing G-Eval with OpenAI\n",
    "\n",
    "Now we put the pieces together: a complete G-Eval implementation that:\n",
    "\n",
    "1. Sends the structured evaluation prompt to the LLM\n",
    "2. Requests `logprobs` to access token probabilities\n",
    "3. Extracts probabilities for score tokens (1-5)\n",
    "4. Computes both discrete and probability-weighted scores\n",
    "\n",
    "**Important API parameters:**\n",
    "- `max_tokens=1` - We only need a single digit output\n",
    "- `logprobs=True` - Enable log probability access\n",
    "- `top_logprobs=5` - Get probabilities for top 5 candidate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def geval_score(document: str, summary: str, model: str = \"gpt-4o-mini\") -> dict:\n",
    "    \"\"\"\n",
    "    G-Eval style scoring with probability weighting.\n",
    "    Returns discrete score and weighted score.\n",
    "    \"\"\"\n",
    "    prompt = GEVAL_COHERENCE_PROMPT.format(document=document, summary=summary)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "    \n",
    "    # Extract token and logprobs\n",
    "    choice = response.choices[0]\n",
    "    discrete_score = int(choice.message.content.strip())\n",
    "    \n",
    "    # Build probability distribution over valid scores\n",
    "    score_probs = {i: 0.0 for i in range(1, 6)}\n",
    "    if choice.logprobs and choice.logprobs.content:\n",
    "        for item in choice.logprobs.content[0].top_logprobs:\n",
    "            token = item.token.strip()\n",
    "            if token in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                score_probs[int(token)] = np.exp(item.logprob)\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    total = sum(score_probs.values())\n",
    "    if total > 0:\n",
    "        score_probs = {k: v / total for k, v in score_probs.items()}\n",
    "    \n",
    "    weighted_score = probability_weighted_score(score_probs)\n",
    "    \n",
    "    return {\n",
    "        \"discrete_score\": discrete_score,\n",
    "        \"weighted_score\": weighted_score,\n",
    "        \"probabilities\": score_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the G-Eval scorer on our example document and summary. Notice how the probability distribution provides insight into the model's confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test G-Eval scoring\n",
    "result = geval_score(document, summary)\n",
    "\n",
    "print(f\"Document: {document[:60]}...\")\n",
    "print(f\"Summary: {summary[:60]}...\")\n",
    "print(f\"\\nDiscrete score: {result['discrete_score']}\")\n",
    "print(f\"Weighted score: {result['weighted_score']:.2f}\")\n",
    "print(f\"\\nProbability distribution:\")\n",
    "for score, prob in result['probabilities'].items():\n",
    "    bar = \"█\" * int(prob * 20)\n",
    "    print(f\"  {score}: {prob:.2%} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Pairwise Comparison\n",
    "\n",
    "While pointwise scoring (rating each response independently) is useful, **pairwise comparison** often achieves higher correlation with human preferences. This is because:\n",
    "\n",
    "1. **Easier judgment** - \"Which is better?\" is often easier than \"Rate this 1-5\"\n",
    "2. **Calibration-free** - No need to calibrate what a \"3\" means\n",
    "3. **Natural for preference data** - Matches how RLHF training data is collected\n",
    "\n",
    "### The Pairwise Judging Approach\n",
    "\n",
    "In pairwise comparison, the LLM receives:\n",
    "- The original question/prompt\n",
    "- Two candidate responses (A and B)\n",
    "- Instructions to compare and choose the better one\n",
    "\n",
    "The prompt below follows the MT-Bench format, a widely-used benchmark for evaluating LLM judges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRWISE_PROMPT = \"\"\"Please act as an impartial judge and evaluate the quality of the \n",
    "responses provided by two AI assistants to the user's question.\n",
    "\n",
    "Your evaluation should consider correctness, helpfulness, and relevance.\n",
    "\n",
    "Avoid any position biases and ensure that the order in which the responses \n",
    "were presented does not influence your decision.\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[Assistant A's Answer]\n",
    "{answer_a}\n",
    "\n",
    "[Assistant B's Answer]\n",
    "{answer_b}\n",
    "\n",
    "After providing your explanation, output your final verdict by strictly \n",
    "following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if \n",
    "assistant B is better, and \"[[C]]\" for a tie.\"\"\"\n",
    "\n",
    "def pairwise_judge(\n",
    "    question: str, \n",
    "    answer_a: str, \n",
    "    answer_b: str,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> dict:\n",
    "    \"\"\"Judge which response is better using pairwise comparison.\"\"\"\n",
    "    prompt = PAIRWISE_PROMPT.format(\n",
    "        question=question,\n",
    "        answer_a=answer_a,\n",
    "        answer_b=answer_b\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    # Extract verdict\n",
    "    if \"[[A]]\" in content:\n",
    "        verdict = \"A\"\n",
    "    elif \"[[B]]\" in content:\n",
    "        verdict = \"B\"\n",
    "    elif \"[[C]]\" in content:\n",
    "        verdict = \"tie\"\n",
    "    else:\n",
    "        verdict = \"unknown\"\n",
    "    \n",
    "    return {\"verdict\": verdict, \"reasoning\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the pairwise judge with two responses of different quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pairwise comparison\n",
    "question = \"What is the capital of France?\"\n",
    "answer_a = \"The capital of France is Paris.\"\n",
    "answer_b = \"Paris is the capital city of France, known for the Eiffel Tower and rich cultural heritage.\"\n",
    "\n",
    "result = pairwise_judge(question, answer_a, answer_b)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAssistant A: {answer_a}\")\n",
    "print(f\"Assistant B: {answer_b}\")\n",
    "print(f\"\\nVerdict: {result['verdict']}\")\n",
    "print(f\"\\nReasoning:\\n{result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigating Position Bias\n",
    "\n",
    "**Position bias** is one of the most significant issues with LLM judges: they tend to prefer responses in certain positions (often the first one) regardless of quality.\n",
    "\n",
    "**The debiasing strategy:**\n",
    "1. Run the comparison with A first, B second\n",
    "2. Run again with B first, A second  \n",
    "3. Only trust the verdict if both runs agree\n",
    "\n",
    "If the verdicts disagree, the comparison is marked \"inconclusive\" - this is better than returning a biased result.\n",
    "\n",
    "**Research findings on position bias:**\n",
    "- GPT-4 shows 10-15% position bias in some benchmarks\n",
    "- Bias varies by task type and response length\n",
    "- Swapping can reduce bias by 50% or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_judge_debiased(\n",
    "    question: str,\n",
    "    answer_a: str,\n",
    "    answer_b: str,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run pairwise comparison twice with swapped positions.\n",
    "    Only return a verdict if both runs agree.\n",
    "    \"\"\"\n",
    "    # Original order: A first, B second\n",
    "    result1 = pairwise_judge(question, answer_a, answer_b, model)\n",
    "    \n",
    "    # Swapped order: B first, A second\n",
    "    result2 = pairwise_judge(question, answer_b, answer_a, model)\n",
    "    \n",
    "    # Map swapped result back\n",
    "    swapped_verdict = {\"A\": \"B\", \"B\": \"A\", \"tie\": \"tie\"}.get(result2[\"verdict\"], \"unknown\")\n",
    "    \n",
    "    # Check agreement\n",
    "    if result1[\"verdict\"] == swapped_verdict:\n",
    "        confident = True\n",
    "        final_verdict = result1[\"verdict\"]\n",
    "    else:\n",
    "        confident = False\n",
    "        final_verdict = \"inconclusive\"\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": final_verdict,\n",
    "        \"confident\": confident,\n",
    "        \"original_order\": result1[\"verdict\"],\n",
    "        \"swapped_order\": result2[\"verdict\"]\n",
    "    }\n",
    "\n",
    "# Test debiased comparison\n",
    "result = pairwise_judge_debiased(question, answer_a, answer_b)\n",
    "print(f\"Original order verdict: {result['original_order']}\")\n",
    "print(f\"Swapped order verdict: {result['swapped_order']} (mapped back)\")\n",
    "print(f\"Final verdict: {result['verdict']}\")\n",
    "print(f\"Confident: {result['confident']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Reference-Guided Grading\n",
    "\n",
    "For **factual or reasoning tasks** (math, coding, knowledge questions), we can improve judge accuracy by providing a reference answer. However, there is a critical issue: **context contamination**.\n",
    "\n",
    "### The Context Contamination Problem\n",
    "\n",
    "If you ask the judge to generate a reference answer and then evaluate candidates in the same context, the judge may:\n",
    "- Favor responses that match its own reasoning style\n",
    "- Be biased toward its own errors\n",
    "- Apply inconsistent standards\n",
    "\n",
    "### The Two-Phase Solution\n",
    "\n",
    "Reference-guided grading separates the process:\n",
    "\n",
    "1. **Phase 1:** Generate reference answer in a clean context (no candidate answers visible)\n",
    "2. **Phase 2:** Compare candidates against the reference in a separate API call\n",
    "\n",
    "This ensures the reference is unbiased by the candidates being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_GUIDED_PROMPT = \"\"\"Please act as an impartial judge and evaluate the quality \n",
    "of the responses provided by two AI assistants to the user's question.\n",
    "\n",
    "Your evaluation should consider correctness and helpfulness. You will be given \n",
    "a reference answer, assistant A's answer, and assistant B's answer.\n",
    "\n",
    "Your job is to evaluate which assistant's answer is better.\n",
    "\n",
    "Begin your evaluation by comparing both assistants' answers with the reference \n",
    "answer. Identify and correct any mistakes.\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[Reference Answer]\n",
    "{reference}\n",
    "\n",
    "[Assistant A's Answer]\n",
    "{answer_a}\n",
    "\n",
    "[Assistant B's Answer]\n",
    "{answer_b}\n",
    "\n",
    "After providing your explanation, output your final verdict: \"[[A]]\" if \n",
    "assistant A is better, \"[[B]]\" if assistant B is better, \"[[C]]\" for a tie.\"\"\"\n",
    "\n",
    "def reference_guided_judge(\n",
    "    question: str,\n",
    "    answer_a: str,\n",
    "    answer_b: str,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Two-phase evaluation:\n",
    "    1. Generate reference answer in clean context\n",
    "    2. Compare candidates against reference\n",
    "    \"\"\"\n",
    "    # Phase 1: Generate reference answer\n",
    "    ref_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    reference = ref_response.choices[0].message.content\n",
    "    \n",
    "    # Phase 2: Compare with reference\n",
    "    prompt = REFERENCE_GUIDED_PROMPT.format(\n",
    "        question=question,\n",
    "        reference=reference,\n",
    "        answer_a=answer_a,\n",
    "        answer_b=answer_b\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    if \"[[A]]\" in content:\n",
    "        verdict = \"A\"\n",
    "    elif \"[[B]]\" in content:\n",
    "        verdict = \"B\"\n",
    "    elif \"[[C]]\" in content:\n",
    "        verdict = \"tie\"\n",
    "    else:\n",
    "        verdict = \"unknown\"\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": verdict,\n",
    "        \"reference\": reference,\n",
    "        \"reasoning\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test reference-guided grading on a math problem where one answer is clearly wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reference-guided grading on a math problem\n",
    "math_question = \"What is 15% of 80?\"\n",
    "answer_correct = \"15% of 80 is 12.\"\n",
    "answer_wrong = \"15% of 80 is 15.\"  # Common mistake\n",
    "\n",
    "result = reference_guided_judge(math_question, answer_correct, answer_wrong)\n",
    "print(f\"Question: {math_question}\")\n",
    "print(f\"\\nAssistant A: {answer_correct}\")\n",
    "print(f\"Assistant B: {answer_wrong}\")\n",
    "print(f\"\\nReference: {result['reference']}\")\n",
    "print(f\"\\nVerdict: {result['verdict']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Structured Output for Reliable Evaluation\n",
    "\n",
    "When building production evaluation systems, you need **consistent, parseable outputs**. JSON structured outputs solve this by constraining the LLM to a predefined schema.\n",
    "\n",
    "### Critical Design Principle: Reasoning Before Score\n",
    "\n",
    "Due to the **autoregressive nature of LLMs**, the order of fields in your schema matters:\n",
    "\n",
    "- **Wrong:** `{\"score\": 4, \"reasoning\": \"...\"}` - Score is generated before reasoning\n",
    "- **Right:** `{\"reasoning\": \"...\", \"score\": 4}` - Reasoning informs the score\n",
    "\n",
    "When reasoning comes first, the LLM \"thinks through\" the evaluation before committing to a score, leading to more thoughtful and consistent judgments.\n",
    "\n",
    "The following uses Pydantic models with OpenAI's structured output feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Schema for structured evaluation output.\"\"\"\n",
    "    reasoning_steps: list[str]  # Must come BEFORE score (autoregressive ordering)\n",
    "    score: int\n",
    "\n",
    "STRUCTURED_PROMPT = \"\"\"Evaluate this summary for coherence on a scale of 1-5.\n",
    "\n",
    "Coherence: The summary should be well-structured, well-organized, and build \n",
    "from sentence to sentence into a coherent body of information.\n",
    "\n",
    "Document: {document}\n",
    "Summary: {summary}\n",
    "\n",
    "Provide step-by-step reasoning, then assign a score.\"\"\"\n",
    "\n",
    "def structured_eval(document: str, summary: str, model: str = \"gpt-4o-mini\") -> dict:\n",
    "    \"\"\"Structured evaluation with reasoning before score.\"\"\"\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": STRUCTURED_PROMPT.format(document=document, summary=summary)\n",
    "        }],\n",
    "        response_format=EvaluationResult\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.parsed\n",
    "    return {\n",
    "        \"reasoning\": result.reasoning_steps,\n",
    "        \"score\": result.score\n",
    "    }\n",
    "\n",
    "# Test structured evaluation\n",
    "result = structured_eval(document, summary)\n",
    "print(\"Reasoning steps:\")\n",
    "for i, step in enumerate(result[\"reasoning\"], 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "print(f\"\\nScore: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Multi-Model Evaluation\n",
    "\n",
    "Different LLMs have different strengths and biases as judges. Using multiple judge models can:\n",
    "- Reduce model-specific biases\n",
    "- Increase evaluation robustness\n",
    "- Provide uncertainty estimates (disagreement indicates hard cases)\n",
    "\n",
    "### Using Claude as a Judge\n",
    "\n",
    "Anthropic's Claude models work with the same prompting patterns. The main API differences are:\n",
    "- Use `messages.create()` instead of `chat.completions.create()`\n",
    "- Response structure differs slightly (access via `response.content[0].text`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "def claude_judge(\n",
    "    question: str,\n",
    "    answer_a: str,\n",
    "    answer_b: str,\n",
    "    model: str = \"claude-sonnet-4-20250514\"\n",
    ") -> dict:\n",
    "    \"\"\"Pairwise comparison using Claude.\"\"\"\n",
    "    prompt = PAIRWISE_PROMPT.format(\n",
    "        question=question,\n",
    "        answer_a=answer_a,\n",
    "        answer_b=answer_b\n",
    "    )\n",
    "    \n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=500,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    content = response.content[0].text\n",
    "    \n",
    "    if \"[[A]]\" in content:\n",
    "        verdict = \"A\"\n",
    "    elif \"[[B]]\" in content:\n",
    "        verdict = \"B\"\n",
    "    elif \"[[C]]\" in content:\n",
    "        verdict = \"tie\"\n",
    "    else:\n",
    "        verdict = \"unknown\"\n",
    "    \n",
    "    return {\"verdict\": verdict, \"reasoning\": content}\n",
    "\n",
    "# Test Claude as judge\n",
    "result = claude_judge(question, answer_a, answer_b)\n",
    "print(f\"Claude's verdict: {result['verdict']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Biases and Limitations of LLM Judges\n",
    "\n",
    "Before relying on LLM judges, understand their key limitations:\n",
    "\n",
    "### Known Biases\n",
    "\n",
    "| Bias | Description | Mitigation |\n",
    "|------|-------------|------------|\n",
    "| **Position bias** | Preference for first or second position | Swap and require agreement |\n",
    "| **Verbosity bias** | Preference for longer responses | Normalize for length; test explicitly |\n",
    "| **Self-enhancement** | LLMs prefer their own outputs | Use different model as judge |\n",
    "| **Authority bias** | Swayed by confident-sounding text | Focus on factual accuracy criteria |\n",
    "| **Style bias** | Preference for certain writing styles | Multi-dimensional rubrics |\n",
    "\n",
    "### When NOT to Use LLM Judges\n",
    "\n",
    "- **Safety-critical decisions** - Human review required\n",
    "- **Legal/compliance** - Regulatory requirements may prohibit\n",
    "- **Novel domains** - LLM may lack expertise\n",
    "- **Adversarial settings** - Can be gamed by crafted responses\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always validate** against human annotations on a held-out set\n",
    "2. **Report confidence intervals** for correlation metrics\n",
    "3. **Test for known biases** before deployment\n",
    "4. **Use multiple judges** for important decisions\n",
    "5. **Monitor for drift** as LLMs update\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Practice implementing and testing LLM judges with these exercises:\n",
    "\n",
    "1. **Compute agreement metrics**: Calculate Spearman's rho and Kendall's tau between two human annotators' scores. When do they disagree?\n",
    "\n",
    "2. **Test for verbosity bias**: Create two responses with identical content but different lengths. Does the judge prefer the longer one?\n",
    "\n",
    "3. **Build a multi-dimensional rubric**: Evaluate responses on helpfulness, accuracy, and tone. Use geometric mean to combine scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Verbosity Bias Test**\n",
    "\n",
    "Test whether your LLM judge exhibits verbosity bias by comparing a concise correct answer to a verbose correct answer. If the judge prefers the verbose answer despite equal correctness, that indicates verbosity bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Verbosity bias test\n",
    "concise_answer = \"The capital of France is Paris.\"\n",
    "verbose_answer = \"\"\"The capital of France is Paris. Paris is a beautiful city \n",
    "located in the north-central part of France. It is known for many famous \n",
    "landmarks including the Eiffel Tower, the Louvre Museum, and Notre-Dame \n",
    "Cathedral. The city has been the capital since the 10th century and remains \n",
    "the political, economic, and cultural center of France today.\"\"\"\n",
    "\n",
    "result = pairwise_judge(\n",
    "    \"What is the capital of France?\",\n",
    "    concise_answer,\n",
    "    verbose_answer\n",
    ")\n",
    "print(f\"Concise: {concise_answer}\")\n",
    "print(f\"Verbose: {verbose_answer[:50]}...\")\n",
    "print(f\"\\nVerdict: {result['verdict']}\")\n",
    "print(\"^ Does the judge exhibit verbosity bias?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **LLM judges are powerful** - They can evaluate subjective qualities that traditional metrics cannot, approximating human judgment at scale\n",
    "\n",
    "2. **Measuring agreement is essential** - Use Spearman's rho for rankings, Kendall's tau for pairwise preferences, and Cohen's kappa for categorical judgments\n",
    "\n",
    "3. **Three main judging approaches:**\n",
    "   - **Pointwise (G-Eval)** - Rate each response independently; use probability weighting for finer granularity\n",
    "   - **Pairwise** - Compare two responses directly; often higher correlation with human preferences\n",
    "   - **Reference-guided** - Use a separate reference answer for factual tasks; avoid context contamination\n",
    "\n",
    "4. **Prompt engineering matters:**\n",
    "   - Define clear evaluation criteria\n",
    "   - Include step-by-step evaluation instructions\n",
    "   - Put reasoning before scores in structured outputs\n",
    "   - Use consistent output formats\n",
    "\n",
    "5. **Biases require active mitigation:**\n",
    "   - Position bias: swap and require agreement\n",
    "   - Verbosity bias: test explicitly; consider length normalization\n",
    "   - Self-enhancement: use different models for generation and judging\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Build an evaluation pipeline for your use case\n",
    "- Collect human annotations to validate your LLM judge\n",
    "- Experiment with different prompt designs and models\n",
    "- Consider ensemble approaches for critical decisions\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634)\n",
    "- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)\n",
    "- [Large Language Models are not Fair Evaluators](https://arxiv.org/abs/2305.17926)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
