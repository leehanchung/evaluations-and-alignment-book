{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: BERTScore and COMET\n",
    "\n",
    "Hands-on implementation of semantic evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Lexical Metrics Fail\n",
    "\n",
    "BLEU/ROUGE treat words as atomic symbols. Synonyms score as poorly as random words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the book\n",
    "reference = \"people like foreign cars\"\n",
    "candidate1 = \"People like visiting places abroad.\"  # Different topic!\n",
    "candidate2 = \"Consumers prefer imported cars.\"       # Same meaning!\n",
    "\n",
    "def word_overlap(cand, ref):\n",
    "    cand_words = set(cand.lower().split())\n",
    "    ref_words = set(ref.lower().split())\n",
    "    return len(cand_words & ref_words) / len(ref_words)\n",
    "\n",
    "print(f\"Reference: '{reference}'\")\n",
    "print(f\"\\nCandidate 1 (wrong topic): '{candidate1}'\")\n",
    "print(f\"  Word overlap: {word_overlap(candidate1, reference):.0%}\")\n",
    "print(f\"\\nCandidate 2 (same meaning): '{candidate2}'\")\n",
    "print(f\"  Word overlap: {word_overlap(candidate2, reference):.0%}\")\n",
    "print(\"\\n^ Lexical metrics penalize valid paraphrases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings: Words as Vectors\n",
    "\n",
    "In embedding space, \"attorney\" and \"lawyer\" are neighbors, not strangers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained encoder model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"Embedding dimension: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text: str) -> torch.Tensor:\n",
    "    \"\"\"Get token embeddings using mean pooling.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Mean pooling over tokens (excluding padding)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = (token_embeddings * mask_expanded).sum(dim=1)\n",
    "    token_counts = mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "    \n",
    "    return sum_embeddings / token_counts\n",
    "\n",
    "def cosine_similarity(emb1: torch.Tensor, emb2: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "    return F.cosine_similarity(emb1, emb2).item()\n",
    "\n",
    "# Test with book examples\n",
    "emb_ref = get_embeddings(reference)\n",
    "emb_c1 = get_embeddings(candidate1)\n",
    "emb_c2 = get_embeddings(candidate2)\n",
    "\n",
    "print(f\"Cosine similarity (semantic):\")\n",
    "print(f\"  Candidate 1 (wrong topic): {cosine_similarity(emb_ref, emb_c1):.3f}\")\n",
    "print(f\"  Candidate 2 (same meaning): {cosine_similarity(emb_ref, emb_c2):.3f}\")\n",
    "print(\"\\n^ Semantic metrics recognize paraphrases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore: Token-Level Greedy Matching\n",
    "\n",
    "For each token in one sentence, find its best semantic match in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embeddings(text: str) -> tuple[torch.Tensor, list[str]]:\n",
    "    \"\"\"Get individual token embeddings.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get tokens (skip [CLS] and [SEP])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])[1:-1]\n",
    "    embeddings = outputs.last_hidden_state[0, 1:-1, :]  # Skip special tokens\n",
    "    \n",
    "    return embeddings, tokens\n",
    "\n",
    "# Example from the book\n",
    "reference = \"The weather is cold today.\"\n",
    "candidate = \"It is freezing today.\"\n",
    "\n",
    "ref_emb, ref_tokens = get_token_embeddings(reference)\n",
    "cand_emb, cand_tokens = get_token_embeddings(candidate)\n",
    "\n",
    "print(f\"Reference tokens: {ref_tokens}\")\n",
    "print(f\"Candidate tokens: {cand_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_cosine_similarity(emb1: torch.Tensor, emb2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute pairwise cosine similarity matrix.\"\"\"\n",
    "    # Normalize embeddings\n",
    "    emb1_norm = F.normalize(emb1, p=2, dim=1)\n",
    "    emb2_norm = F.normalize(emb2, p=2, dim=1)\n",
    "    # Compute similarity matrix\n",
    "    return torch.mm(emb1_norm, emb2_norm.t())\n",
    "\n",
    "# Compute similarity matrix\n",
    "sim_matrix = pairwise_cosine_similarity(ref_emb, cand_emb)\n",
    "\n",
    "print(\"Pairwise similarity matrix (reference × candidate):\")\n",
    "print(f\"Shape: {sim_matrix.shape} ({len(ref_tokens)} ref × {len(cand_tokens)} cand)\\n\")\n",
    "\n",
    "# Display as table\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(\n",
    "    sim_matrix.numpy(),\n",
    "    index=ref_tokens,\n",
    "    columns=cand_tokens\n",
    ")\n",
    "print(df.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertscore_components(ref_emb: torch.Tensor, cand_emb: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate BERTScore precision, recall, and F1.\n",
    "    \n",
    "    Recall: For each reference token, find best match in candidate.\n",
    "    Precision: For each candidate token, find best match in reference.\n",
    "    \"\"\"\n",
    "    sim_matrix = pairwise_cosine_similarity(ref_emb, cand_emb)\n",
    "    \n",
    "    # Recall: max over candidate for each reference token\n",
    "    recall_scores = sim_matrix.max(dim=1).values\n",
    "    recall = recall_scores.mean().item()\n",
    "    \n",
    "    # Precision: max over reference for each candidate token\n",
    "    precision_scores = sim_matrix.max(dim=0).values\n",
    "    precision = precision_scores.mean().item()\n",
    "    \n",
    "    # F1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"recall_scores\": recall_scores,\n",
    "        \"precision_scores\": precision_scores\n",
    "    }\n",
    "\n",
    "result = bertscore_components(ref_emb, cand_emb)\n",
    "\n",
    "print(\"BERTScore (unscaled):\")\n",
    "print(f\"  Precision: {result['precision']:.3f}\")\n",
    "print(f\"  Recall: {result['recall']:.3f}\")\n",
    "print(f\"  F1: {result['f1']:.3f}\")\n",
    "\n",
    "print(\"\\nRecall breakdown (best match for each reference token):\")\n",
    "for token, score in zip(ref_tokens, result['recall_scores']):\n",
    "    print(f\"  {token:12} -> {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Matching Visualization\n",
    "\n",
    "Which candidate token best matches each reference token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best matches\n",
    "best_matches = sim_matrix.argmax(dim=1)\n",
    "\n",
    "print(\"Greedy matching (reference -> best candidate match):\")\n",
    "print(\"-\" * 45)\n",
    "for i, (ref_tok, best_idx) in enumerate(zip(ref_tokens, best_matches)):\n",
    "    cand_tok = cand_tokens[best_idx]\n",
    "    score = sim_matrix[i, best_idx].item()\n",
    "    print(f\"  '{ref_tok}' -> '{cand_tok}' (similarity: {score:.3f})\")\n",
    "\n",
    "print(\"\\n^ 'cold' matches 'freezing' despite no lexical overlap!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `bert_score` Library\n",
    "\n",
    "In practice, use the optimized implementation with IDF weighting and baseline rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as bert_score\n",
    "\n",
    "# Book examples\n",
    "references = [\n",
    "    \"The weather is cold today.\",\n",
    "    \"people like foreign cars\"\n",
    "]\n",
    "candidates = [\n",
    "    \"It is freezing today.\",\n",
    "    \"Consumers prefer imported cars.\"\n",
    "]\n",
    "\n",
    "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=False)\n",
    "\n",
    "print(\"BERTScore (with IDF weighting & rescaling):\")\n",
    "print(\"-\" * 50)\n",
    "for i, (cand, ref) in enumerate(zip(candidates, references)):\n",
    "    print(f\"Reference: '{ref}'\")\n",
    "    print(f\"Candidate: '{cand}'\")\n",
    "    print(f\"  P={P[i]:.3f}, R={R[i]:.3f}, F1={F1[i]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## COMET: Learned Evaluation from Human Judgments\n\nCOMET uses three inputs: source, candidate, and reference. Unlike BERTScore which uses pretrained embeddings directly, COMET is **trained on human judgment data** from WMT translation tasks.\n\n> **Note:** COMET (`unbabel-comet`) requires `transformers<5.0`. If you're using Python 3.14+ with newer transformers, install COMET in a separate environment:\n> ```bash\n> uv venv comet-env --python 3.12\n> source comet-env/bin/activate\n> pip install unbabel-comet\n> ```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMET requires transformers<5.0 - this cell may fail in newer environments\ntry:\n    from comet import download_model, load_from_checkpoint\n    \n    # Download and load COMET model\n    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n    comet_model = load_from_checkpoint(model_path)\n    COMET_AVAILABLE = True\nexcept ImportError:\n    print(\"COMET not available. Install with: pip install unbabel-comet\")\n    print(\"Requires transformers<5.0 and Python<3.14\")\n    COMET_AVAILABLE = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example from the book: German bank translation\ndata = [\n    {\n        \"src\": \"Die Bank war voller Kunden.\",\n        \"mt\": \"The bank was full of customers.\",\n        \"ref\": \"The financial institution was crowded with clients.\"\n    },\n    {\n        \"src\": \"Die Bank war voller Kunden.\",\n        \"mt\": \"The riverside was full of customers.\",  # Wrong sense of \"Bank\"!\n        \"ref\": \"The financial institution was crowded with clients.\"\n    }\n]\n\nif COMET_AVAILABLE:\n    output = comet_model.predict(data, batch_size=2, gpus=0)\n    \n    print(\"COMET scores (trained on human judgments):\")\n    print(\"-\" * 50)\n    for i, (sample, score) in enumerate(zip(data, output.scores)):\n        print(f\"Source: {sample['src']}\")\n        print(f\"MT: {sample['mt']}\")\n        print(f\"Ref: {sample['ref']}\")\n        print(f\"COMET Score: {score:.3f}\\n\")\nelse:\n    print(\"COMET Example (requires separate environment):\")\n    print(\"-\" * 50)\n    for sample in data:\n        print(f\"Source: {sample['src']}\")\n        print(f\"MT: {sample['mt']}\")\n        print(f\"Ref: {sample['ref']}\")\n        print()\n    print(\"^ COMET would score the first translation higher\")\n    print(\"  because it correctly translates 'Bank' as 'bank' (financial)\")\n    print(\"  while the second mistranslates it as 'riverside'.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Test case: paraphrase with no word overlap\n",
    "reference = \"The attorney filed the legal brief.\"\n",
    "candidate = \"The lawyer submitted the court document.\"\n",
    "\n",
    "print(f\"Reference: '{reference}'\")\n",
    "print(f\"Candidate: '{candidate}'\")\n",
    "print(\"\\nMetric comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# BLEU\n",
    "bleu_result = bleu.compute(predictions=[candidate], references=[[reference]])\n",
    "print(f\"BLEU:      {bleu_result['bleu']:.3f}\")\n",
    "\n",
    "# ROUGE\n",
    "rouge_result = rouge.compute(predictions=[candidate], references=[reference])\n",
    "print(f\"ROUGE-L:   {rouge_result['rougeL']:.3f}\")\n",
    "\n",
    "# BERTScore\n",
    "bert_result = bertscore.compute(\n",
    "    predictions=[candidate], \n",
    "    references=[reference], \n",
    "    lang=\"en\"\n",
    ")\n",
    "print(f\"BERTScore: {bert_result['f1'][0]:.3f}\")\n",
    "\n",
    "print(\"\\n^ Semantic metrics capture paraphrase equivalence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Compute BERTScore for: \"The committee approved the proposal yesterday after extensive debate.\" vs \"Yesterday, following lengthy discussions, the proposal received committee approval.\"\n",
    "\n",
    "2. Why might COMET give different scores than BERTScore for the same translation?\n",
    "\n",
    "3. When would you prefer lexical metrics (BLEU/ROUGE) over semantic metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "ref = \"The committee approved the proposal yesterday after extensive debate.\"\n",
    "cand = \"Yesterday, following lengthy discussions, the proposal received committee approval.\"\n",
    "\n",
    "P, R, F1 = bert_score([cand], [ref], lang=\"en\", verbose=False)\n",
    "print(f\"BERTScore F1: {F1[0]:.3f}\")\n",
    "print(\"High score despite completely different word order!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}