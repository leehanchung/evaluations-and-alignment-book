{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: BERTScore and COMET\n",
        "\n",
        "## Semantic Evaluation Metrics for NLG\n",
        "\n",
        "---\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. **Explain** why lexical metrics (BLEU, ROUGE) fail to capture semantic equivalence\n",
        "2. **Understand** how word embeddings represent meaning as vectors in high-dimensional space\n",
        "3. **Implement** BERTScore from scratch using greedy token matching\n",
        "4. **Use** the `bert_score` library for production-quality evaluation\n",
        "5. **Apply** COMET for translation evaluation using source, candidate, and reference\n",
        "6. **Choose** the right metric for your specific NLG evaluation task\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "Traditional metrics like BLEU and ROUGE rely on **lexical overlap** - they count matching n-grams between candidate and reference texts. This approach has a fundamental flaw: it treats words as atomic symbols with no relationship to each other.\n",
        "\n",
        "Consider these sentences:\n",
        "- \"The **lawyer** submitted the **brief**\"\n",
        "- \"The **attorney** filed the **document**\"\n",
        "\n",
        "These sentences mean nearly the same thing, but share almost no words! Lexical metrics would score this valid paraphrase as a failure. **Semantic metrics** solve this by understanding that \"lawyer\" and \"attorney\" are synonyms, even though they have different spellings.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "We start by importing the necessary libraries:\n",
        "\n",
        "- **PyTorch**: For tensor operations and neural network computations\n",
        "- **Transformers**: Hugging Face library for pretrained language models like BERT\n",
        "- **NumPy**: For numerical operations\n",
        "\n",
        "These tools give us access to pretrained models that have learned rich semantic representations from massive text corpora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: The Problem with Lexical Metrics\n",
        "\n",
        "### Why BLEU and ROUGE Fail\n",
        "\n",
        "BLEU and ROUGE treat words as **atomic symbols** - like random ID numbers with no inherent meaning. In this view:\n",
        "- \"cat\" and \"dog\" are just as different as \"cat\" and \"quantum\"\n",
        "- \"lawyer\" and \"attorney\" are completely unrelated symbols\n",
        "\n",
        "This is fundamentally at odds with how language works. Synonyms, paraphrases, and semantically equivalent expressions are everywhere in natural language.\n",
        "\n",
        "The following example demonstrates this limitation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example from the book\n",
        "reference = \"people like foreign cars\"\n",
        "candidate1 = \"People like visiting places abroad.\"  # Different topic!\n",
        "candidate2 = \"Consumers prefer imported cars.\"       # Same meaning!\n",
        "\n",
        "def word_overlap(cand, ref):\n",
        "    cand_words = set(cand.lower().split())\n",
        "    ref_words = set(ref.lower().split())\n",
        "    return len(cand_words & ref_words) / len(ref_words)\n",
        "\n",
        "print(f\"Reference: '{reference}'\")\n",
        "print(f\"\\nCandidate 1 (wrong topic): '{candidate1}'\")\n",
        "print(f\"  Word overlap: {word_overlap(candidate1, reference):.0%}\")\n",
        "print(f\"\\nCandidate 2 (same meaning): '{candidate2}'\")\n",
        "print(f\"  Word overlap: {word_overlap(candidate2, reference):.0%}\")\n",
        "print(\"\\n^ Lexical metrics penalize valid paraphrases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting the Results\n",
        "\n",
        "Notice the paradox above:\n",
        "- **Candidate 1** (\"people like visiting places abroad\") shares 50% of words but talks about **travel**, not cars\n",
        "- **Candidate 2** (\"Consumers prefer imported cars\") shares 0% of words but means **exactly the same thing**\n",
        "\n",
        "A lexical metric would prefer Candidate 1, which is semantically wrong! This is why we need metrics that understand **meaning**, not just **spelling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: The Solution - Word Embeddings\n",
        "\n",
        "### From Symbols to Vectors\n",
        "\n",
        "The breakthrough insight of modern NLP is representing words as **dense vectors** (embeddings) in a high-dimensional space. In this space:\n",
        "\n",
        "- **Similar words are neighbors**: \"attorney\" and \"lawyer\" are close together\n",
        "- **Different concepts are far apart**: \"attorney\" and \"pizza\" are distant\n",
        "- **Relationships are directions**: king - man + woman ≈ queen\n",
        "\n",
        "**How are embeddings learned?**\n",
        "\n",
        "Models like BERT learn embeddings by predicting masked words from context. After training on billions of words, the model learns that \"attorney\" and \"lawyer\" appear in similar contexts, so they get similar vectors.\n",
        "\n",
        "**Contextual embeddings go further:**\n",
        "\n",
        "Unlike static embeddings (Word2Vec), BERT produces **contextual embeddings** - the vector for \"bank\" differs between \"river bank\" and \"bank account\". This allows nuanced semantic understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading a Pretrained Encoder\n",
        "\n",
        "We use BERT (Bidirectional Encoder Representations from Transformers) as our embedding model. BERT-base has:\n",
        "- 12 transformer layers\n",
        "- 768-dimensional embeddings\n",
        "- 110 million parameters\n",
        "\n",
        "The model was pretrained on Wikipedia and BookCorpus, learning rich semantic representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pretrained encoder model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded {model_name}\")\n",
        "print(f\"Embedding dimension: {model.config.hidden_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentence-Level Semantic Similarity\n",
        "\n",
        "Now we can compare sentences by their meaning, not their spelling. The code below:\n",
        "\n",
        "1. **Gets embeddings** for each sentence using mean pooling (averaging all token embeddings)\n",
        "2. **Computes cosine similarity** between the sentence vectors\n",
        "\n",
        "**Cosine similarity** measures the angle between two vectors:\n",
        "- 1.0 = identical direction (same meaning)\n",
        "- 0.0 = perpendicular (unrelated)\n",
        "- -1.0 = opposite direction (opposite meaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embeddings(text: str) -> torch.Tensor:\n",
        "    \"\"\"Get token embeddings using mean pooling.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Mean pooling over tokens (excluding padding)\n",
        "    token_embeddings = outputs.last_hidden_state\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = (token_embeddings * mask_expanded).sum(dim=1)\n",
        "    token_counts = mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
        "    \n",
        "    return sum_embeddings / token_counts\n",
        "\n",
        "def cosine_similarity(emb1: torch.Tensor, emb2: torch.Tensor) -> float:\n",
        "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
        "    return F.cosine_similarity(emb1, emb2).item()\n",
        "\n",
        "# Test with book examples\n",
        "emb_ref = get_embeddings(reference)\n",
        "emb_c1 = get_embeddings(candidate1)\n",
        "emb_c2 = get_embeddings(candidate2)\n",
        "\n",
        "print(f\"Cosine similarity (semantic):\")\n",
        "print(f\"  Candidate 1 (wrong topic): {cosine_similarity(emb_ref, emb_c1):.3f}\")\n",
        "print(f\"  Candidate 2 (same meaning): {cosine_similarity(emb_ref, emb_c2):.3f}\")\n",
        "print(\"\\n^ Semantic metrics recognize paraphrases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: BERTScore - Token-Level Semantic Matching\n",
        "\n",
        "### The BERTScore Algorithm\n",
        "\n",
        "While sentence-level similarity is useful, **BERTScore** provides a more nuanced approach by matching individual tokens. The algorithm:\n",
        "\n",
        "1. **Get token embeddings** from BERT for both reference and candidate\n",
        "2. **Compute pairwise similarities** between all reference-candidate token pairs\n",
        "3. **Greedy matching**: For each token, find its best semantic match in the other sentence\n",
        "4. **Aggregate** into Precision, Recall, and F1 scores\n",
        "\n",
        "**Why greedy matching?**\n",
        "\n",
        "Instead of requiring exact word matches, BERTScore finds the best semantic match for each token. \"Cold\" can match \"freezing\" because their embeddings are similar, even though the words are different.\n",
        "\n",
        "**Precision vs Recall:**\n",
        "- **Recall**: How much of the reference meaning is captured? (For each reference token, find best match in candidate)\n",
        "- **Precision**: How accurate is the candidate? (For each candidate token, find best match in reference)\n",
        "\n",
        "Let's implement this step by step:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Extract Token Embeddings\n",
        "\n",
        "First, we tokenize our sentences and extract embeddings for each token. Note that we skip the special `[CLS]` and `[SEP]` tokens that BERT adds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_token_embeddings(text: str) -> tuple[torch.Tensor, list[str]]:\n",
        "    \"\"\"Get individual token embeddings.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Get tokens (skip [CLS] and [SEP])\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])[1:-1]\n",
        "    embeddings = outputs.last_hidden_state[0, 1:-1, :]  # Skip special tokens\n",
        "    \n",
        "    return embeddings, tokens\n",
        "\n",
        "# Example from the book\n",
        "reference = \"The weather is cold today.\"\n",
        "candidate = \"It is freezing today.\"\n",
        "\n",
        "ref_emb, ref_tokens = get_token_embeddings(reference)\n",
        "cand_emb, cand_tokens = get_token_embeddings(candidate)\n",
        "\n",
        "print(f\"Reference tokens: {ref_tokens}\")\n",
        "print(f\"Candidate tokens: {cand_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Build the Similarity Matrix\n",
        "\n",
        "Now we compute cosine similarity between every pair of tokens. This creates a matrix where:\n",
        "- Rows = reference tokens\n",
        "- Columns = candidate tokens\n",
        "- Each cell = semantic similarity between that token pair\n",
        "\n",
        "Look for high values (near 1.0) to see which tokens are semantically similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pairwise_cosine_similarity(emb1: torch.Tensor, emb2: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute pairwise cosine similarity matrix.\"\"\"\n",
        "    # Normalize embeddings\n",
        "    emb1_norm = F.normalize(emb1, p=2, dim=1)\n",
        "    emb2_norm = F.normalize(emb2, p=2, dim=1)\n",
        "    # Compute similarity matrix\n",
        "    return torch.mm(emb1_norm, emb2_norm.t())\n",
        "\n",
        "# Compute similarity matrix\n",
        "sim_matrix = pairwise_cosine_similarity(ref_emb, cand_emb)\n",
        "\n",
        "print(\"Pairwise similarity matrix (reference × candidate):\")\n",
        "print(f\"Shape: {sim_matrix.shape} ({len(ref_tokens)} ref × {len(cand_tokens)} cand)\\n\")\n",
        "\n",
        "# Display as table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(\n",
        "    sim_matrix.numpy(),\n",
        "    index=ref_tokens,\n",
        "    columns=cand_tokens\n",
        ")\n",
        "print(df.round(3).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Compute Precision, Recall, and F1\n",
        "\n",
        "With the similarity matrix, we can now compute BERTScore:\n",
        "\n",
        "- **Recall**: For each reference token, take the maximum similarity across all candidate tokens, then average\n",
        "- **Precision**: For each candidate token, take the maximum similarity across all reference tokens, then average\n",
        "- **F1**: Harmonic mean of precision and recall\n",
        "\n",
        "This greedy maximum matching allows \"cold\" to find \"freezing\" as its best match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bertscore_components(ref_emb: torch.Tensor, cand_emb: torch.Tensor) -> dict:\n",
        "    \"\"\"\n",
        "    Calculate BERTScore precision, recall, and F1.\n",
        "    \n",
        "    Recall: For each reference token, find best match in candidate.\n",
        "    Precision: For each candidate token, find best match in reference.\n",
        "    \"\"\"\n",
        "    sim_matrix = pairwise_cosine_similarity(ref_emb, cand_emb)\n",
        "    \n",
        "    # Recall: max over candidate for each reference token\n",
        "    recall_scores = sim_matrix.max(dim=1).values\n",
        "    recall = recall_scores.mean().item()\n",
        "    \n",
        "    # Precision: max over reference for each candidate token\n",
        "    precision_scores = sim_matrix.max(dim=0).values\n",
        "    precision = precision_scores.mean().item()\n",
        "    \n",
        "    # F1\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"recall_scores\": recall_scores,\n",
        "        \"precision_scores\": precision_scores\n",
        "    }\n",
        "\n",
        "result = bertscore_components(ref_emb, cand_emb)\n",
        "\n",
        "print(\"BERTScore (unscaled):\")\n",
        "print(f\"  Precision: {result['precision']:.3f}\")\n",
        "print(f\"  Recall: {result['recall']:.3f}\")\n",
        "print(f\"  F1: {result['f1']:.3f}\")\n",
        "\n",
        "print(\"\\nRecall breakdown (best match for each reference token):\")\n",
        "for token, score in zip(ref_tokens, result['recall_scores']):\n",
        "    print(f\"  {token:12} -> {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Greedy Matching\n",
        "\n",
        "Let's see exactly which candidate token each reference token matched with. This visualization helps build intuition for how BERTScore captures semantic equivalence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best matches\n",
        "best_matches = sim_matrix.argmax(dim=1)\n",
        "\n",
        "print(\"Greedy matching (reference -> best candidate match):\")\n",
        "print(\"-\" * 45)\n",
        "for i, (ref_tok, best_idx) in enumerate(zip(ref_tokens, best_matches)):\n",
        "    cand_tok = cand_tokens[best_idx]\n",
        "    score = sim_matrix[i, best_idx].item()\n",
        "    print(f\"  '{ref_tok}' -> '{cand_tok}' (similarity: {score:.3f})\")\n",
        "\n",
        "print(\"\\n^ 'cold' matches 'freezing' despite no lexical overlap!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Insight: Semantic Matching in Action\n",
        "\n",
        "The matching above shows the power of semantic metrics:\n",
        "- **\"cold\" -> \"freezing\"**: Different words, same meaning - high similarity!\n",
        "- **\"weather\" -> \"it\"**: Both refer to general conditions\n",
        "- **\"today\" -> \"today\"**: Exact match gets highest similarity\n",
        "\n",
        "This is impossible with lexical metrics, which would see \"cold\" and \"freezing\" as completely different symbols."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Production BERTScore with the `bert_score` Library\n",
        "\n",
        "### Why Use the Library?\n",
        "\n",
        "Our implementation above demonstrates the core algorithm, but the `bert_score` library adds important enhancements:\n",
        "\n",
        "1. **IDF weighting**: Important words (like \"attorney\") contribute more than common words (like \"the\")\n",
        "2. **Baseline rescaling**: Raw scores are rescaled to be more interpretable (0 = random, 1 = perfect)\n",
        "3. **Batching and GPU support**: Efficient processing of large datasets\n",
        "4. **Model selection**: Choose from different encoder models (RoBERTa, DeBERTa, etc.)\n",
        "\n",
        "For production evaluation, always use the library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bert_score import score as bert_score\n",
        "\n",
        "# Book examples\n",
        "references = [\n",
        "    \"The weather is cold today.\",\n",
        "    \"people like foreign cars\"\n",
        "]\n",
        "candidates = [\n",
        "    \"It is freezing today.\",\n",
        "    \"Consumers prefer imported cars.\"\n",
        "]\n",
        "\n",
        "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=False)\n",
        "\n",
        "print(\"BERTScore (with IDF weighting & rescaling):\")\n",
        "print(\"-\" * 50)\n",
        "for i, (cand, ref) in enumerate(zip(candidates, references)):\n",
        "    print(f\"Reference: '{ref}'\")\n",
        "    print(f\"Candidate: '{cand}'\")\n",
        "    print(f\"  P={P[i]:.3f}, R={R[i]:.3f}, F1={F1[i]:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Scores\n",
        "\n",
        "Notice that the library scores differ from our raw implementation due to IDF weighting and rescaling. The rescaled scores are more interpretable:\n",
        "- **F1 > 0.9**: Excellent semantic match\n",
        "- **F1 0.8-0.9**: Good paraphrase\n",
        "- **F1 < 0.7**: Significant semantic differences\n",
        "\n",
        "Both examples above get high scores because they are valid paraphrases, demonstrating that BERTScore correctly captures semantic equivalence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: COMET - Learned Evaluation from Human Judgments\n",
        "\n",
        "### What Makes COMET Different?\n",
        "\n",
        "COMET (Crosslingual Optimized Metric for Evaluation of Translation) takes a fundamentally different approach:\n",
        "\n",
        "| Aspect | BERTScore | COMET |\n",
        "|--------|-----------|-------|\n",
        "| **Inputs** | Candidate + Reference | Source + Candidate + Reference |\n",
        "| **Model** | Pretrained encoder (BERT) | Fine-tuned on human judgments |\n",
        "| **Training** | No task-specific training | Trained to predict human scores |\n",
        "| **Use case** | General NLG | Translation evaluation |\n",
        "\n",
        "**Key advantages of COMET:**\n",
        "\n",
        "1. **Source awareness**: By seeing the original source text, COMET can detect translation errors that lose meaning (like mistranslating \"Bank\" as \"riverside\" instead of \"financial institution\")\n",
        "\n",
        "2. **Human-aligned**: COMET is trained on millions of human quality judgments from WMT translation tasks, so it learns what humans consider good translations\n",
        "\n",
        "3. **State-of-the-art correlation**: COMET consistently achieves the highest correlation with human judgments on translation tasks\n",
        "\n",
        "> **Note:** COMET requires `transformers<5.0` which may conflict with newer Python versions. See the installation note below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the COMET Model\n",
        "\n",
        "COMET models are large (~1.5GB) and require specific package versions. The code below attempts to load the WMT22 COMET-DA model, which was trained on Direct Assessment scores from WMT translation tasks.\n",
        "\n",
        "> **Installation Note:** COMET (`unbabel-comet`) requires `transformers<5.0`. If you're using Python 3.14+ with newer transformers, install COMET in a separate environment:\n",
        "> ```bash\n",
        "> uv venv comet-env --python 3.12\n",
        "> source comet-env/bin/activate\n",
        "> pip install unbabel-comet\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMET requires transformers<5.0 - this cell may fail in newer environments\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "    \n",
        "    # Download and load COMET model\n",
        "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "    comet_model = load_from_checkpoint(model_path)\n",
        "    COMET_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"COMET not available. Install with: pip install unbabel-comet\")\n",
        "    print(\"Requires transformers<5.0 and Python<3.14\")\n",
        "    COMET_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMET in Action: Detecting Translation Errors\n",
        "\n",
        "This example demonstrates COMET's key strength: using the source text to detect meaning-changing errors.\n",
        "\n",
        "The German word \"Bank\" is ambiguous - it can mean:\n",
        "- **Financial institution** (the correct translation in this context)\n",
        "- **Riverside/bench** (incorrect translation)\n",
        "\n",
        "COMET, by seeing the source \"Die Bank war voller Kunden\" (The bank was full of customers), can determine that \"bank\" (financial) is correct and \"riverside\" is wrong, even though both are valid English sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example from the book: German bank translation\n",
        "data = [\n",
        "    {\n",
        "        \"src\": \"Die Bank war voller Kunden.\",\n",
        "        \"mt\": \"The bank was full of customers.\",\n",
        "        \"ref\": \"The financial institution was crowded with clients.\"\n",
        "    },\n",
        "    {\n",
        "        \"src\": \"Die Bank war voller Kunden.\",\n",
        "        \"mt\": \"The riverside was full of customers.\",  # Wrong sense of \"Bank\"!\n",
        "        \"ref\": \"The financial institution was crowded with clients.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "if COMET_AVAILABLE:\n",
        "    output = comet_model.predict(data, batch_size=2, gpus=0)\n",
        "    \n",
        "    print(\"COMET scores (trained on human judgments):\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, (sample, score) in enumerate(zip(data, output.scores)):\n",
        "        print(f\"Source: {sample['src']}\")\n",
        "        print(f\"MT: {sample['mt']}\")\n",
        "        print(f\"Ref: {sample['ref']}\")\n",
        "        print(f\"COMET Score: {score:.3f}\\n\")\n",
        "else:\n",
        "    print(\"COMET Example (requires separate environment):\")\n",
        "    print(\"-\" * 50)\n",
        "    for sample in data:\n",
        "        print(f\"Source: {sample['src']}\")\n",
        "        print(f\"MT: {sample['mt']}\")\n",
        "        print(f\"Ref: {sample['ref']}\")\n",
        "        print()\n",
        "    print(\"^ COMET would score the first translation higher\")\n",
        "    print(\"  because it correctly translates 'Bank' as 'bank' (financial)\")\n",
        "    print(\"  while the second mistranslates it as 'riverside'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Comparing All Metrics\n",
        "\n",
        "### Head-to-Head Comparison\n",
        "\n",
        "Now let's compare lexical and semantic metrics on the same example. This demonstrates why choosing the right metric matters for your evaluation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "# Test case: paraphrase with no word overlap\n",
        "reference = \"The attorney filed the legal brief.\"\n",
        "candidate = \"The lawyer submitted the court document.\"\n",
        "\n",
        "print(f\"Reference: '{reference}'\")\n",
        "print(f\"Candidate: '{candidate}'\")\n",
        "print(\"\\nMetric comparison:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# BLEU\n",
        "bleu_result = bleu.compute(predictions=[candidate], references=[[reference]])\n",
        "print(f\"BLEU:      {bleu_result['bleu']:.3f}\")\n",
        "\n",
        "# ROUGE\n",
        "rouge_result = rouge.compute(predictions=[candidate], references=[reference])\n",
        "print(f\"ROUGE-L:   {rouge_result['rougeL']:.3f}\")\n",
        "\n",
        "# BERTScore\n",
        "bert_result = bertscore.compute(\n",
        "    predictions=[candidate], \n",
        "    references=[reference], \n",
        "    lang=\"en\"\n",
        ")\n",
        "print(f\"BERTScore: {bert_result['f1'][0]:.3f}\")\n",
        "\n",
        "print(\"\\n^ Semantic metrics capture paraphrase equivalence!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting the Comparison\n",
        "\n",
        "The results above clearly show the difference:\n",
        "\n",
        "| Metric | Score | Why |\n",
        "|--------|-------|-----|\n",
        "| **BLEU** | ~0 | No n-gram overlap between \"attorney/lawyer\" or \"filed/submitted\" |\n",
        "| **ROUGE-L** | ~0 | No common subsequence |\n",
        "| **BERTScore** | ~0.9+ | Recognizes semantic equivalence via embeddings |\n",
        "\n",
        "This is a perfect paraphrase that lexical metrics completely miss!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercises\n",
        "\n",
        "Test your understanding of semantic evaluation metrics with these exercises.\n",
        "\n",
        "1. **Compute BERTScore** for the paraphrase pair below. Notice the completely different word order.\n",
        "\n",
        "2. **Why might COMET give different scores than BERTScore** for the same translation?\n",
        "   - Hint: Think about what information each metric has access to.\n",
        "\n",
        "3. **When would you prefer lexical metrics (BLEU/ROUGE)** over semantic metrics?\n",
        "   - Hint: Consider computation cost, interpretability, and specific task requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: BERTScore on Paraphrases\n",
        "\n",
        "The sentences below are paraphrases with completely reorganized structure. Run the code to see how BERTScore handles this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1\n",
        "ref = \"The committee approved the proposal yesterday after extensive debate.\"\n",
        "cand = \"Yesterday, following lengthy discussions, the proposal received committee approval.\"\n",
        "\n",
        "P, R, F1 = bert_score([cand], [ref], lang=\"en\", verbose=False)\n",
        "print(f\"BERTScore F1: {F1[0]:.3f}\")\n",
        "print(\"High score despite completely different word order!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Lexical metrics (BLEU, ROUGE) fail on paraphrases** because they treat words as atomic symbols with no semantic relationship\n",
        "\n",
        "2. **Word embeddings represent meaning as vectors** where semantically similar words are neighbors in the embedding space\n",
        "\n",
        "3. **BERTScore uses greedy token matching** to find the best semantic match for each token, enabling paraphrase recognition\n",
        "\n",
        "4. **COMET is trained on human judgments** and uses source text for translation evaluation, achieving state-of-the-art correlation with humans\n",
        "\n",
        "### When to Use Each Metric\n",
        "\n",
        "| Metric | Best For | Limitations |\n",
        "|--------|----------|-------------|\n",
        "| **BLEU** | MT systems with consistent output style; fast corpus-level evaluation | Penalizes valid paraphrases; needs multiple references |\n",
        "| **ROUGE** | Summarization; recall-oriented tasks | Same lexical limitations as BLEU |\n",
        "| **BERTScore** | General NLG; paraphrase-tolerant evaluation | Computationally expensive; no source awareness |\n",
        "| **COMET** | Translation evaluation; when source is available | Requires source text; large model size |\n",
        "\n",
        "### Strengths of Semantic Metrics\n",
        "\n",
        "- Recognize synonyms and paraphrases (\"attorney\" ≈ \"lawyer\")\n",
        "- Capture semantic similarity even with different word order\n",
        "- Better correlation with human judgments\n",
        "- Contextual understanding (for BERT-based metrics)\n",
        "\n",
        "### Limitations to Keep in Mind\n",
        "\n",
        "- **Computational cost**: Require GPU for efficient processing of large datasets\n",
        "- **Model bias**: Inherit biases from pretraining data\n",
        "- **Not interpretable**: Harder to understand why a score is low (vs. counting n-grams)\n",
        "- **Calibration varies**: Raw scores need rescaling to be meaningful across models\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- [BERTScore Paper](https://arxiv.org/abs/1904.09675) - Zhang et al., 2019\n",
        "- [COMET Paper](https://arxiv.org/abs/2009.09025) - Rei et al., 2020\n",
        "- [BLEURT](https://arxiv.org/abs/2004.04696) - Another learned metric worth exploring\n",
        "\n",
        "---\n",
        "\n",
        "*End of Chapter 3*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
