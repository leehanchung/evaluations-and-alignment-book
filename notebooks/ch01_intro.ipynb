{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Evaluations and Alignments for AI\n",
    "\n",
    "Hands-on examples to accompany Chapter 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifiable Task Example: Code Evaluation\n",
    "\n",
    "HumanEval-style evaluation where tests determine correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_close_elements(numbers: list[float], threshold: float) -> bool:\n",
    "    \"\"\"Check if any two numbers are closer than threshold.\"\"\"\n",
    "    for i, num1 in enumerate(numbers):\n",
    "        for j, num2 in enumerate(numbers):\n",
    "            if i != j and abs(num1 - num2) < threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Verifiable: tests pass or fail\n",
    "tests = [\n",
    "    ([1.0, 2.0, 3.0], 0.5, False),\n",
    "    ([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3, True),\n",
    "]\n",
    "\n",
    "for nums, thresh, expected in tests:\n",
    "    result = has_close_elements(nums, thresh)\n",
    "    status = \"✓\" if result == expected else \"✗\"\n",
    "    print(f\"{status} has_close_elements({nums}, {thresh}) = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Answer Extraction\n",
    "\n",
    "Extracting answers from chain-of-thought responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"Let me work through this. 42 x 38 = 1680 - 84 = 1596. The answer is 1,596.\",\n",
    "    \"1596\",\n",
    "    \"The answer is 1,596\",\n",
    "]\n",
    "\n",
    "def extract_number(text: str) -> int | None:\n",
    "    \"\"\"Extract the final number from a response.\"\"\"\n",
    "    text = text.replace(\",\", \"\")\n",
    "    numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "    return int(numbers[-1]) if numbers else None\n",
    "\n",
    "for r in responses:\n",
    "    print(f\"{extract_number(r)} <- '{r[:40]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness Metric\n",
    "\n",
    "Measuring hallucination by checking claim support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfulness(claims: list[dict]) -> float:\n",
    "    \"\"\"Faithfulness = Supported Claims / Total Claims\"\"\"\n",
    "    if not claims:\n",
    "        return 1.0\n",
    "    return sum(c['supported'] for c in claims) / len(claims)\n",
    "\n",
    "# Example: AI response about an arXiv paper\n",
    "claims = [\n",
    "    {\"claim\": \"Year: 2020\", \"supported\": True},\n",
    "    {\"claim\": \"Source: arXiv\", \"supported\": True},\n",
    "    {\"claim\": \"Title: 'GLU Variants Improve Transformer'\", \"supported\": True},\n",
    "    {\"claim\": \"arXiv: 2002.05202v1\", \"supported\": True},\n",
    "    {\"claim\": \"Authors: Narang, Chung\", \"supported\": False},  # Hallucinated!\n",
    "]\n",
    "\n",
    "score = faithfulness(claims)\n",
    "print(f\"Faithfulness: {score:.0%}\")\n",
    "print(f\"Hallucination rate: {1-score:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Classify these tasks as verifiable or open-ended:\n",
    "   - Sentiment classification\n",
    "   - Poetry generation\n",
    "   - SQL query generation\n",
    "   - Customer support response\n",
    "\n",
    "2. An AI response has 12 claims, 3 unsupported. Calculate faithfulness.\n",
    "\n",
    "3. Design a scenario where policy alignment conflicts with principled alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "total, unsupported = 12, 3\n",
    "print(f\"Faithfulness: {(total - unsupported) / total:.0%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
