{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Evaluations and Alignments for AI\n",
    "\n",
    "Welcome to the hands-on companion for Chapter 1! This notebook introduces the fundamental concepts of AI evaluation and alignment through practical examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Distinguish between verifiable and open-ended tasks** - Understand why some AI outputs can be automatically checked while others require human judgment\n",
    "2. **Implement basic evaluation patterns** - Write code to test AI-generated solutions against ground truth\n",
    "3. **Extract structured answers from free-form text** - Handle the practical challenge of parsing AI responses\n",
    "4. **Calculate faithfulness metrics** - Quantify hallucination rates in AI-generated content\n",
    "5. **Recognize the tension between different alignment goals** - Understand why \"aligned AI\" isn't a simple concept\n",
    "\n",
    "---\n",
    "\n",
    "## Why Evaluation Matters\n",
    "\n",
    "Before an AI system can be deployed, we need to answer a fundamental question: **Is it working correctly?**\n",
    "\n",
    "This seems simple, but AI evaluation is surprisingly nuanced:\n",
    "- For a calculator, \"correct\" means matching the mathematical answer\n",
    "- For a chatbot, \"correct\" might mean helpful, harmless, and honest\n",
    "- For a creative writing assistant, \"correct\" is subjective\n",
    "\n",
    "This chapter explores the spectrum from **verifiable tasks** (clear right/wrong answers) to **open-ended tasks** (requiring human judgment), and introduces the concept of **faithfulness** as a way to measure hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the libraries we'll use throughout this notebook. These are standard tools for data manipulation, numerical computing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Verifiable Tasks\n",
    "\n",
    "### What Makes a Task \"Verifiable\"?\n",
    "\n",
    "A **verifiable task** is one where correctness can be determined automatically, without human judgment. Examples include:\n",
    "\n",
    "- **Code generation**: Does the code pass unit tests?\n",
    "- **Math problems**: Does the answer match the solution?\n",
    "- **Factual questions**: Is the response correct according to a knowledge base?\n",
    "- **Classification**: Does the label match the ground truth?\n",
    "\n",
    "The key insight is that verifiable tasks have a **ground truth** that can be checked programmatically.\n",
    "\n",
    "### HumanEval: A Benchmark for Code Generation\n",
    "\n",
    "[HumanEval](https://github.com/openai/human-eval) is a famous benchmark that evaluates AI code generation. Each problem includes:\n",
    "1. A function signature with docstring\n",
    "2. A set of hidden test cases\n",
    "\n",
    "The AI's job is to complete the function. Success is binary: either all tests pass, or they don't.\n",
    "\n",
    "Below is an example problem. Notice how we can definitively say whether the implementation is correct by running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_close_elements(numbers: list[float], threshold: float) -> bool:\n",
    "    \"\"\"Check if any two numbers are closer than threshold.\"\"\"\n",
    "    for i, num1 in enumerate(numbers):\n",
    "        for j, num2 in enumerate(numbers):\n",
    "            if i != j and abs(num1 - num2) < threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Verifiable: tests pass or fail\n",
    "tests = [\n",
    "    ([1.0, 2.0, 3.0], 0.5, False),\n",
    "    ([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3, True),\n",
    "]\n",
    "\n",
    "for nums, thresh, expected in tests:\n",
    "    result = has_close_elements(nums, thresh)\n",
    "    status = \"\\u2713\" if result == expected else \"\\u2717\"\n",
    "    print(f\"{status} has_close_elements({nums}, {thresh}) = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway: Verifiable Evaluation\n",
    "\n",
    "Notice what just happened:\n",
    "- We had a **clear specification** (the docstring)\n",
    "- We had **test cases** with expected outputs\n",
    "- We could **automatically determine** if the code was correct\n",
    "\n",
    "This is the gold standard for AI evaluation: no ambiguity, no human judgment needed. However, most real-world AI applications don't fit this mold neatly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Answer Extraction Problem\n",
    "\n",
    "### From Reasoning to Evaluation\n",
    "\n",
    "Modern AI models often \"think out loud\" using **chain-of-thought (CoT)** reasoning. While this improves accuracy, it creates a practical challenge: **how do we extract the final answer for evaluation?**\n",
    "\n",
    "Consider asking an AI: \"What is 42 x 38?\"\n",
    "\n",
    "The AI might respond:\n",
    "> \"Let me work through this step by step. 42 x 38 can be computed as 42 x 40 - 42 x 2 = 1680 - 84 = 1596. Therefore, the answer is 1,596.\"\n",
    "\n",
    "The reasoning is helpful for understanding, but for evaluation we need to extract just \"1596\" to compare against the ground truth.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Answer extraction seems trivial but causes real problems in evaluation:\n",
    "- Models might format numbers differently (1596 vs 1,596 vs \"one thousand five hundred ninety-six\")\n",
    "- The answer might appear multiple times in the reasoning\n",
    "- Different models have different response styles\n",
    "\n",
    "Robust answer extraction is essential for fair, reproducible evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [\n",
    "    \"Let me work through this. 42 x 38 = 1680 - 84 = 1596. The answer is 1,596.\",\n",
    "    \"1596\",\n",
    "    \"The answer is 1,596\",\n",
    "]\n",
    "\n",
    "def extract_number(text: str) -> int | None:\n",
    "    \"\"\"Extract the final number from a response.\"\"\"\n",
    "    text = text.replace(\",\", \"\")\n",
    "    numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "    return int(numbers[-1]) if numbers else None\n",
    "\n",
    "for r in responses:\n",
    "    print(f\"{extract_number(r)} <- '{r[:40]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Extraction Works\n",
    "\n",
    "Our simple extractor uses a common heuristic: **take the last number in the response**. This works because models typically state their final answer at the end.\n",
    "\n",
    "The regex pattern `\\b\\d+\\b` matches word-boundary-delimited digit sequences, and we remove commas first to handle formatted numbers like \"1,596\".\n",
    "\n",
    "> **Note**: Production systems often use more sophisticated extraction, including asking the model to format answers in a specific way (e.g., \"Put your final answer in \\\\boxed{}\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Measuring Faithfulness (Detecting Hallucination)\n",
    "\n",
    "### The Hallucination Problem\n",
    "\n",
    "One of the most significant challenges with large language models is **hallucination**: generating content that sounds plausible but is factually incorrect. This is especially dangerous when:\n",
    "\n",
    "- Users trust the AI's confident tone\n",
    "- The AI is used for research or decision-making\n",
    "- Incorrect information could cause harm\n",
    "\n",
    "### What is Faithfulness?\n",
    "\n",
    "**Faithfulness** measures how well an AI's response is grounded in source material. It answers the question: \"Of all the claims the AI made, how many are actually supported by the evidence?\"\n",
    "\n",
    "$$\\text{Faithfulness} = \\frac{\\text{Supported Claims}}{\\text{Total Claims}}$$\n",
    "\n",
    "A faithfulness score of 100% means every claim can be verified. Lower scores indicate hallucination.\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Imagine asking an AI to summarize an arXiv paper. The AI makes several claims about the paper. We can check each claim against the actual paper to compute faithfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfulness(claims: list[dict]) -> float:\n",
    "    \"\"\"Faithfulness = Supported Claims / Total Claims\"\"\"\n",
    "    if not claims:\n",
    "        return 1.0\n",
    "    return sum(c['supported'] for c in claims) / len(claims)\n",
    "\n",
    "# Example: AI response about an arXiv paper\n",
    "claims = [\n",
    "    {\"claim\": \"Year: 2020\", \"supported\": True},\n",
    "    {\"claim\": \"Source: arXiv\", \"supported\": True},\n",
    "    {\"claim\": \"Title: 'GLU Variants Improve Transformer'\", \"supported\": True},\n",
    "    {\"claim\": \"arXiv: 2002.05202v1\", \"supported\": True},\n",
    "    {\"claim\": \"Authors: Narang, Chung\", \"supported\": False},  # Hallucinated!\n",
    "]\n",
    "\n",
    "score = faithfulness(claims)\n",
    "print(f\"Faithfulness: {score:.0%}\")\n",
    "print(f\"Hallucination rate: {1-score:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "In this example, the AI correctly identified the year, source, title, and arXiv ID, but **hallucinated the authors**. This is a common failure mode: the AI might know the paper exists but confuse details with similar papers.\n",
    "\n",
    "With 4 out of 5 claims supported, we get:\n",
    "- **Faithfulness**: 80%\n",
    "- **Hallucination rate**: 20%\n",
    "\n",
    "> **Key Insight**: A single hallucinated fact can undermine trust in an entire response. This is why faithfulness evaluation is critical for applications like research assistance, medical information, or legal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "Now it's your turn! These exercises reinforce the concepts from this chapter.\n",
    "\n",
    "### Exercise 1: Task Classification\n",
    "\n",
    "Classify each of the following tasks as **verifiable** or **open-ended**:\n",
    "\n",
    "| Task | Verifiable or Open-ended? | Why? |\n",
    "|------|---------------------------|------|\n",
    "| Sentiment classification | ? | ? |\n",
    "| Poetry generation | ? | ? |\n",
    "| SQL query generation | ? | ? |\n",
    "| Customer support response | ? | ? |\n",
    "\n",
    "*Think about: Can you automatically check if the output is \"correct\"?*\n",
    "\n",
    "### Exercise 2: Calculate Faithfulness\n",
    "\n",
    "An AI response about a historical event contains 12 claims. After fact-checking, you find that 3 claims are not supported by the source material.\n",
    "\n",
    "Calculate the faithfulness score using the code cell below.\n",
    "\n",
    "### Exercise 3: Alignment Conflicts (Discussion)\n",
    "\n",
    "Design a scenario where **policy alignment** (following the organization's rules) conflicts with **principled alignment** (doing what's ethically right).\n",
    "\n",
    "*Hint: Consider cases involving user privacy, content moderation, or access to information.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "total, unsupported = 12, 3\n",
    "print(f\"Faithfulness: {(total - unsupported) / total:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Verifiable vs. Open-ended Tasks**\n",
    "   - Verifiable tasks have ground truth that can be checked automatically\n",
    "   - Open-ended tasks require human judgment or more sophisticated evaluation\n",
    "   - The distinction matters for choosing evaluation strategies\n",
    "\n",
    "2. **Answer Extraction is Non-trivial**\n",
    "   - Chain-of-thought reasoning improves AI accuracy but complicates evaluation\n",
    "   - Robust extraction handles formatting variations and locates final answers\n",
    "   - Evaluation infrastructure must be as rigorous as the models themselves\n",
    "\n",
    "3. **Faithfulness Quantifies Hallucination**\n",
    "   - Faithfulness = Supported Claims / Total Claims\n",
    "   - Even high faithfulness (e.g., 80%) means some claims are wrong\n",
    "   - Critical for trust in AI-generated content\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "In the following chapters, we'll explore specific evaluation metrics:\n",
    "- **Chapter 2**: BLEU and ROUGE for text generation quality\n",
    "- **Chapter 3**: BERTScore and COMET for semantic similarity\n",
    "- **Chapter 4**: LLM-as-a-Judge for automated open-ended evaluation\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook is part of the AI Evaluation and Alignments book (Manning Seminal Papers Series).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
